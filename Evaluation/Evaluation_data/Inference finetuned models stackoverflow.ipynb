{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1224787d-0c6e-4d0b-8c43-a290605320e7",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e238df-4a49-4160-877a-9fc6f5ba5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/zealous_easley/.local/lib/python3.8/site-packages (0.1.99)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# we use the latest version of transformers, peft, and accelerate\n",
    "!pip install -q accelerate peft transformers\n",
    "\n",
    "# install bitsandbytes for quantization\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "# install trl for the SFT library\n",
    "!pip install -q trl\n",
    "\n",
    "# we need sentencepiece for the llama2 slow tokenizer\n",
    "!pip install sentencepiece\n",
    "\n",
    "# we need einops, used by falcon-7b, llama-2 etc\n",
    "# einops (einsteinops) is used to simplify tensorops by making them readable\n",
    "!pip install -q -U einops\n",
    "\n",
    "# we need to install datasets for our training dataset\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a44e3-f953-4991-9803-17e36888abe6",
   "metadata": {},
   "source": [
    "# Loading finetuned models and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264baf91-1748-4beb-8279-39958003e003",
   "metadata": {},
   "source": [
    "# Testing the fine tuned model Mistral7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232a805-ad9f-4d90-ac21-198165f52a4c",
   "metadata": {},
   "source": [
    "Inference with mistral7b model finetuned on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae084db-b017-456e-9fc7-ad7335ac19c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 19:58:48.022422: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-13 19:58:48.084905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd39c73970454d9384efde8df317ea3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f227a7b419d0433982acad7bcfb85144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2e8a308ef6424c9225a5c7daa9e54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a06fc2cacd46428997c617b514b0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3abe504164434eb1391e08c758b1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebd075065fc4e16a316e3968aa4bc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4290ed93c8b4492a229007a09014150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b507b512bf4620825eba1bc8196d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1046b3f295438b8faf7b666fb1174f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec251d9fdb84071907ee60d4bd0a28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91d1085b2af4100879b8b5a3bee1e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9826ecad57134564a6998998e3adc597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# Specify the model and tokenizer name from the Hugging Face model hub\n",
    "model_name = \"ayman56/mistral7b_finetuned_full_stackoverflow_test\"\n",
    "tokenizer_name  = \"ayman56/mistral7b_finetuned_full_stackoverflow_test\"\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (mistral)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7fc663-fe2e-4cce-bebc-dcfb94eda113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3296"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86b72e7-b757-4e98-931d-8207dfa216e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Prompts:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   1%|          | 1/100 [00:07<12:06,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   2%|▏         | 2/100 [00:21<18:58, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   3%|▎         | 3/100 [00:59<38:01, 23.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   4%|▍         | 4/100 [01:30<42:07, 26.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   5%|▌         | 5/100 [01:58<42:38, 26.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   6%|▌         | 6/100 [02:32<45:59, 29.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   7%|▋         | 7/100 [03:03<46:21, 29.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   8%|▊         | 8/100 [03:26<42:37, 27.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   9%|▉         | 9/100 [03:56<43:17, 28.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  10%|█         | 10/100 [04:27<43:50, 29.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  11%|█         | 11/100 [04:51<40:52, 27.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  12%|█▏        | 12/100 [05:13<38:09, 26.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  13%|█▎        | 13/100 [05:44<39:41, 27.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  14%|█▍        | 14/100 [06:15<40:41, 28.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  15%|█▌        | 15/100 [06:40<38:50, 27.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  16%|█▌        | 16/100 [07:03<36:33, 26.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  17%|█▋        | 17/100 [07:34<38:20, 27.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  18%|█▊        | 18/100 [08:06<39:29, 28.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  19%|█▉        | 19/100 [08:37<39:45, 29.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  20%|██        | 20/100 [09:02<37:38, 28.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  21%|██        | 21/100 [09:21<33:29, 25.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  22%|██▏       | 22/100 [09:48<33:46, 25.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  23%|██▎       | 23/100 [10:11<32:01, 24.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  24%|██▍       | 24/100 [10:37<32:14, 25.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  25%|██▌       | 25/100 [10:40<23:15, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  26%|██▌       | 26/100 [11:04<25:02, 20.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  27%|██▋       | 27/100 [11:24<24:25, 20.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  28%|██▊       | 28/100 [11:48<25:36, 21.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  29%|██▉       | 29/100 [11:59<21:30, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  30%|███       | 30/100 [12:29<25:29, 21.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  31%|███       | 31/100 [13:00<28:11, 24.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  32%|███▏      | 32/100 [13:25<28:04, 24.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  33%|███▎      | 33/100 [13:42<24:47, 22.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  34%|███▍      | 34/100 [14:11<26:41, 24.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  35%|███▌      | 35/100 [14:42<28:35, 26.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  36%|███▌      | 36/100 [14:58<24:47, 23.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  37%|███▋      | 37/100 [15:22<24:45, 23.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  38%|███▊      | 38/100 [15:46<24:15, 23.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  39%|███▉      | 39/100 [16:12<24:45, 24.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  40%|████      | 40/100 [16:35<23:59, 23.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  41%|████      | 41/100 [16:53<21:51, 22.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  42%|████▏     | 42/100 [17:20<22:50, 23.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  43%|████▎     | 43/100 [17:49<23:49, 25.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  44%|████▍     | 44/100 [18:20<25:05, 26.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  45%|████▌     | 45/100 [18:44<23:47, 25.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  46%|████▌     | 46/100 [19:10<23:28, 26.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  47%|████▋     | 47/100 [19:40<24:12, 27.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  48%|████▊     | 48/100 [20:11<24:34, 28.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  49%|████▉     | 49/100 [20:38<23:41, 27.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  50%|█████     | 50/100 [20:59<21:42, 26.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  51%|█████     | 51/100 [21:24<20:54, 25.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  52%|█████▏    | 52/100 [21:38<17:38, 22.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  53%|█████▎    | 53/100 [22:05<18:32, 23.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  54%|█████▍    | 54/100 [22:31<18:35, 24.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  55%|█████▌    | 55/100 [22:57<18:30, 24.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  56%|█████▌    | 56/100 [23:27<19:16, 26.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  57%|█████▋    | 57/100 [23:56<19:32, 27.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  58%|█████▊    | 58/100 [24:24<19:12, 27.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  59%|█████▉    | 59/100 [24:46<17:39, 25.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  60%|██████    | 60/100 [25:18<18:22, 27.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  61%|██████    | 61/100 [25:47<18:18, 28.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  62%|██████▏   | 62/100 [26:09<16:35, 26.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  63%|██████▎   | 63/100 [26:28<14:50, 24.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  64%|██████▍   | 64/100 [26:35<11:21, 18.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  65%|██████▌   | 65/100 [26:49<10:15, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  66%|██████▌   | 66/100 [27:10<10:25, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  67%|██████▋   | 67/100 [27:33<11:01, 20.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  68%|██████▊   | 68/100 [28:00<11:45, 22.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  69%|██████▉   | 69/100 [28:31<12:44, 24.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  70%|███████   | 70/100 [28:55<12:16, 24.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  71%|███████   | 71/100 [29:27<12:50, 26.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  72%|███████▏  | 72/100 [29:40<10:35, 22.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  73%|███████▎  | 73/100 [29:49<08:23, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  74%|███████▍  | 74/100 [29:58<06:50, 15.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  75%|███████▌  | 75/100 [30:28<08:19, 19.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  76%|███████▌  | 76/100 [30:55<08:46, 21.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  77%|███████▋  | 77/100 [31:22<09:00, 23.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  78%|███████▊  | 78/100 [31:40<08:02, 21.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  79%|███████▉  | 79/100 [31:55<06:58, 19.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  80%|████████  | 80/100 [32:16<06:41, 20.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  81%|████████  | 81/100 [32:28<05:38, 17.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  82%|████████▏ | 82/100 [33:00<06:35, 21.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  83%|████████▎ | 83/100 [33:29<06:49, 24.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  84%|████████▍ | 84/100 [34:00<06:58, 26.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  85%|████████▌ | 85/100 [34:29<06:45, 27.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  86%|████████▌ | 86/100 [34:57<06:21, 27.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  87%|████████▋ | 87/100 [35:19<05:33, 25.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  88%|████████▊ | 88/100 [35:26<04:01, 20.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  89%|████████▉ | 89/100 [35:28<02:40, 14.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  90%|█████████ | 90/100 [35:57<03:09, 18.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  91%|█████████ | 91/100 [36:10<02:35, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  92%|█████████▏| 92/100 [36:15<01:48, 13.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  93%|█████████▎| 93/100 [36:43<02:05, 17.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  94%|█████████▍| 94/100 [36:51<01:28, 14.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  95%|█████████▌| 95/100 [37:20<01:36, 19.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  96%|█████████▌| 96/100 [37:41<01:18, 19.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  97%|█████████▋| 97/100 [38:08<01:05, 21.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  98%|█████████▊| 98/100 [38:37<00:47, 23.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  99%|█████████▉| 99/100 [38:55<00:22, 22.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts: 100%|██████████| 100/100 [39:18<00:00, 23.58s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Define a function to perform inference on a single question\n",
    "def infer_question(prompt):\n",
    "    # Run text generation pipeline with your model\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "    result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Load the test set from the CSV file\n",
    "test_set = pd.read_csv('test_set.csv')\n",
    "\n",
    "# Initialize lists to collect generated prompts, original questions, and original answers\n",
    "generated_prompts = []\n",
    "original_questions = []\n",
    "original_answers = []\n",
    "\n",
    "# Iterate over each row in the test set with tqdm to track progress\n",
    "for index, row in tqdm(test_set.iterrows(), total=len(test_set), desc=\"Generating Prompts\"):\n",
    "    question = row['question']\n",
    "    answer = row['answer']\n",
    "    \n",
    "    # Generate a prompt using the inference function\n",
    "    generated_prompt = infer_question(question)\n",
    "    \n",
    "    # Collect the generated prompt, original question, and original answer\n",
    "    generated_prompts.append(generated_prompt)\n",
    "    original_questions.append(question)\n",
    "    original_answers.append(answer)\n",
    "\n",
    "# Create a DataFrame to store the generated prompts, original questions, and original answers\n",
    "generated_prompts_dataset = pd.DataFrame({\n",
    "    'Generated_Prompt': generated_prompts,\n",
    "    'Original_Question': original_questions,\n",
    "    'Original_Answer': original_answers\n",
    "})\n",
    "\n",
    "# Save the generated prompts dataset to a CSV file\n",
    "generated_prompts_dataset.to_csv('generated_Mistral7b_finetuned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "324e4b8f-30b9-4819-898e-ec2631ac2bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] I\\'m looking for a library in Python which will provide at and cron like functionality.\\nI\\'d quite like have a pure Python solution, rather than relying on tools installed on the box; this way I run on machines with no cron.\\nFor those unfamiliar with cron: you can schedule tasks based upon an expression like: \\n 0 2 * * 7 /usr/bin/run-backup # run the backups at 0200 on Every Sunday\\n 0 9-17/2 * * 1-5 /usr/bin/purge-temps # run the purge temps command, every 2 hours between 9am and 5pm on Mondays to Fridays.\\n\\nThe cron time expression syntax is less important, but I would like to have something with this sort of flexibility. \\nIf there isn\\'t something that does this for me out-the-box, any suggestions for the building blocks to make something like this would be gratefully received.\\nEdit\\nI\\'m not interested in launching processes, just \"jobs\" also written in Python - python functions. By necessity I think this would be a different thread, but not in a different process.\\nTo this end, I\\'m looking for the expressivity of the cron time expression, but in Python. \\nCron has been around for years, but I\\'m trying to be as portable as possible. I cannot rely on its presence.\\n [/INST] I\\'d recommend using the built-in cron. You can set up your jobs using cronw wrapper as shown below:\\nimport cronwrapper\\n\\ndef xxx():\\n    # do stuff\\n    print \"xxx\"\\n\\ndef xxx2():\\n    # do stuff\\n    print \"xxx\"\\n\\n#xx is a placeholder for something, you can also specify it as a string like this:\\n 0 1 * * 2 * * 7 /usr/bin/run-backup # run the backup\\n 0 2 * * 2 * 7 /usr/bin/run-backup # run the backup\\n 0 2 * 7 /usr/bin/run-backup # run the backup\\n\\nYou can also specify the cron-like expressions as shown here:\\n 3 /usr/bin/run-backup # run the backup\\n 0 2 * 7 /usr/bin/run-backup # run the backup\\n 3 2 * 7 /usr/bin/run-backup # run the backup\\n\\nYou can obviously customize the expressions based on your specific situation, but I\\'d recommend following the patterns here:\\n$ xxx * 7 /usr/bin/run-backup # run the backup\\n 0 2 * 7 /usr/bin/run-backup # run the backup\\n 0 2 * 7 /usr/bin/run-backup # run the backup\\n 0 2 * 7 /usr/bin/run-backup # run the backup\\n\\nThe cron-like expression can be modified to suit your specific situation, but the patterns here are what you\\'ll probably want to use.\\nYou\\'ll also need to ensure that the cron-like expressions are written in a way that the shell can parse and execute them, and that the expressions are well-formed. For my own project I\\'ll have expressions that \"promise\" the results, but I\\'ll also want to perform an action based on an expression and then \"express\" the result.\\nHere\\'s a Python implementation of the expression \"1,2,7 /usr/bin/run-backup # run-backup\"\\nimport os\\n\\ndef run_backup():\\n    # run the backup\\n    os.system(\"run-backup\")\\n\\ndef run_backup():\\n    # run the backup\\n    os.system(\"run-backup\")\\n\\nimport os\\n\\ndef run_backup():\\n    # run-backup\\n    # run-backup\\n    # run-backup\\n\\ndef run-backup():\\n    # run-backup\\n    # perform an action\\n\\nimport os\\n\\ndef run-backup():\\n    # perform an action\\n    # \\n    # print \"some output\"\\n\\n\\ndef run-backup():\\n    # perform an action\\n    # perform an action\\n\\nFollowing is the expected result of the expression \"0 2 * 7 /usr/bin/run-backup\" (without the backup)\\n#xx \"some-expression\" # \"some-expression\"\\n#xx \"some-expression\" # \"some-expression\"\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_prompts_dataset.Generated_Prompt.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4421c1a-8b59-4c45-b977-766204bc1179",
   "metadata": {},
   "source": [
    "Inference with mistral7b model finetuned on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92668919-e925-4637-ad3b-686df2e28c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d597a2c1c3422a9a3a765d04c1cc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# Specify the model and tokenizer name from the Hugging Face model hub\n",
    "model_name = \"ayman56/mistral7b_finetuned_60_stackoverflow_test\"\n",
    "tokenizer_name  = \"ayman56/mistral7b_finetuned_60_stackoverflow_test\"\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (mistral)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "734e98de-eb24-49ed-b240-6261f52358e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Prompts:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   1%|          | 1/100 [00:38<1:03:43, 38.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   2%|▏         | 2/100 [01:23<1:08:49, 42.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   3%|▎         | 3/100 [02:26<1:23:50, 51.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   4%|▍         | 4/100 [03:27<1:28:33, 55.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   5%|▌         | 5/100 [04:28<1:30:59, 57.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   6%|▌         | 6/100 [05:32<1:33:21, 59.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   7%|▋         | 7/100 [06:35<1:34:07, 60.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   8%|▊         | 8/100 [07:30<1:30:37, 59.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   9%|▉         | 9/100 [08:33<1:31:23, 60.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  10%|█         | 10/100 [09:38<1:32:34, 61.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  11%|█         | 11/100 [10:34<1:29:02, 60.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  12%|█▏        | 12/100 [11:29<1:25:45, 58.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  13%|█▎        | 13/100 [12:32<1:26:48, 59.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  14%|█▍        | 14/100 [13:36<1:27:28, 61.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  15%|█▌        | 15/100 [14:36<1:25:52, 60.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  16%|█▌        | 16/100 [15:32<1:22:57, 59.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  17%|█▋        | 17/100 [16:37<1:24:16, 60.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  18%|█▊        | 18/100 [17:41<1:24:47, 62.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  19%|█▉        | 19/100 [18:45<1:24:29, 62.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  20%|██        | 20/100 [19:43<1:21:41, 61.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  21%|██        | 21/100 [20:35<1:16:43, 58.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  22%|██▏       | 22/100 [21:36<1:17:03, 59.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  23%|██▎       | 23/100 [22:32<1:14:29, 58.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  24%|██▍       | 24/100 [23:30<1:13:43, 58.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  25%|██▌       | 25/100 [24:05<1:04:04, 51.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  26%|██▌       | 26/100 [25:03<1:05:48, 53.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  27%|██▋       | 27/100 [25:58<1:05:11, 53.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  28%|██▊       | 28/100 [26:55<1:05:46, 54.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  29%|██▉       | 29/100 [27:39<1:00:57, 51.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  30%|███       | 30/100 [28:43<1:04:20, 55.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  31%|███       | 31/100 [29:46<1:06:11, 57.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  32%|███▏      | 32/100 [30:45<1:05:49, 58.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  33%|███▎      | 33/100 [31:33<1:01:35, 55.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  34%|███▍      | 34/100 [32:35<1:02:37, 56.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  35%|███▌      | 35/100 [33:38<1:03:41, 58.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  36%|███▌      | 36/100 [34:26<59:15, 55.55s/it]  Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  37%|███▋      | 37/100 [35:20<57:49, 55.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  38%|███▊      | 38/100 [36:13<56:25, 54.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  39%|███▉      | 39/100 [37:11<56:29, 55.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  40%|████      | 40/100 [38:06<55:23, 55.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  41%|████      | 41/100 [38:58<53:31, 54.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  42%|████▏     | 42/100 [39:57<53:54, 55.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  43%|████▎     | 43/100 [40:57<54:11, 57.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  44%|████▍     | 44/100 [42:00<54:46, 58.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  45%|████▌     | 45/100 [42:55<52:56, 57.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  46%|████▌     | 46/100 [43:53<52:03, 57.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  47%|████▋     | 47/100 [44:55<52:14, 59.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  48%|████▊     | 48/100 [45:57<52:01, 60.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  49%|████▉     | 49/100 [46:55<50:19, 59.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  50%|█████     | 50/100 [47:45<47:05, 56.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  51%|█████     | 51/100 [48:42<46:13, 56.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  52%|█████▏    | 52/100 [49:28<42:43, 53.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  53%|█████▎    | 53/100 [50:27<43:08, 55.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  54%|█████▍    | 54/100 [51:25<42:52, 55.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  55%|█████▌    | 55/100 [52:23<42:25, 56.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  56%|█████▌    | 56/100 [53:25<42:43, 58.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  57%|█████▋    | 57/100 [54:28<42:41, 59.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  58%|█████▊    | 58/100 [55:28<41:56, 59.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  59%|█████▉    | 59/100 [56:23<39:50, 58.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  60%|██████    | 60/100 [57:27<39:58, 59.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  61%|██████    | 61/100 [58:29<39:23, 60.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  62%|██████▏   | 62/100 [59:23<37:15, 58.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  63%|██████▎   | 63/100 [1:00:15<34:53, 56.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  64%|██████▍   | 64/100 [1:00:54<30:49, 51.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  65%|██████▌   | 65/100 [1:01:41<29:10, 50.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  66%|██████▌   | 66/100 [1:02:33<28:47, 50.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  67%|██████▋   | 67/100 [1:03:30<28:53, 52.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  68%|██████▊   | 68/100 [1:04:30<29:09, 54.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  69%|██████▉   | 69/100 [1:05:34<29:40, 57.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  70%|███████   | 70/100 [1:06:31<28:40, 57.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  71%|███████   | 71/100 [1:07:35<28:39, 59.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  72%|███████▏  | 72/100 [1:08:21<25:55, 55.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  73%|███████▎  | 73/100 [1:09:03<23:08, 51.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  74%|███████▍  | 74/100 [1:09:45<21:00, 48.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  75%|███████▌  | 75/100 [1:10:47<21:58, 52.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  76%|███████▌  | 76/100 [1:11:47<21:53, 54.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  77%|███████▋  | 77/100 [1:12:47<21:33, 56.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  78%|███████▊  | 78/100 [1:13:37<19:58, 54.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  79%|███████▉  | 79/100 [1:14:25<18:21, 52.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  80%|████████  | 80/100 [1:15:18<17:34, 52.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  81%|████████  | 81/100 [1:16:03<16:00, 50.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  82%|████████▏ | 82/100 [1:17:08<16:27, 54.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  83%|████████▎ | 83/100 [1:18:11<16:10, 57.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  84%|████████▍ | 84/100 [1:19:17<15:58, 59.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  85%|████████▌ | 85/100 [1:20:25<15:35, 62.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  86%|████████▌ | 86/100 [1:21:26<14:24, 61.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  87%|████████▋ | 87/100 [1:22:20<12:56, 59.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  88%|████████▊ | 88/100 [1:23:02<10:50, 54.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  89%|████████▉ | 89/100 [1:23:37<08:52, 48.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  90%|█████████ | 90/100 [1:24:38<08:42, 52.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  91%|█████████ | 91/100 [1:25:24<07:33, 50.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  92%|█████████▏| 92/100 [1:26:01<06:11, 46.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  93%|█████████▎| 93/100 [1:27:03<05:57, 51.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  94%|█████████▍| 94/100 [1:27:44<04:47, 47.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  95%|█████████▌| 95/100 [1:28:47<04:22, 52.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  96%|█████████▌| 96/100 [1:29:42<03:32, 53.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  97%|█████████▋| 97/100 [1:30:41<02:45, 55.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  98%|█████████▊| 98/100 [1:31:43<01:54, 57.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  99%|█████████▉| 99/100 [1:32:35<00:55, 55.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts: 100%|██████████| 100/100 [1:33:31<00:00, 56.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Define a function to perform inference on a single question\n",
    "def infer_question(prompt):\n",
    "    # Run text generation pipeline with your model\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)\n",
    "    result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Load the test set from the CSV file\n",
    "test_set = pd.read_csv('test_set.csv')\n",
    "\n",
    "# Initialize lists to collect generated prompts, original questions, and original answers\n",
    "generated_prompts = []\n",
    "original_questions = []\n",
    "original_answers = []\n",
    "\n",
    "# Iterate over each row in the test set with tqdm to track progress\n",
    "for index, row in tqdm(test_set.iterrows(), total=len(test_set), desc=\"Generating Prompts\"):\n",
    "    question = row['question']\n",
    "    answer = row['answer']\n",
    "    \n",
    "    # Generate a prompt using the inference function\n",
    "    generated_prompt = infer_question(question)\n",
    "    \n",
    "    # Collect the generated prompt, original question, and original answer\n",
    "    generated_prompts.append(generated_prompt)\n",
    "    original_questions.append(question)\n",
    "    original_answers.append(answer)\n",
    "\n",
    "# Create a DataFrame to store the generated prompts, original questions, and original answers\n",
    "generated_prompts_dataset = pd.DataFrame({\n",
    "    'Generated_Prompt': generated_prompts,\n",
    "    'Original_Question': original_questions,\n",
    "    'Original_Answer': original_answers\n",
    "})\n",
    "\n",
    "# Save the generated prompts dataset to a CSV file\n",
    "generated_prompts_dataset.to_csv('generated_prompts_finetune60_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca0ad5-8d52-49d5-af87-3d6ed0287c8c",
   "metadata": {},
   "source": [
    "# Testing the fine tuned model LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68195207-26f4-4777-91b5-8d4d1697f703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11becc41a1c24a8e93e09c53bc5b4287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e904a6871b4771b221f3087d0b3151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ba70120a484c3896903c52e354487c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49313dfb9f14ea39fbd21b540f211bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f562041184284e709b506fa04fa048b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c54ddc3c4444e1b69a88559f7d071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937d5041bb0f4e0bae2179e8bb8a6a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef29356bb2d242b28760778577001fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# Specify the model and tokenizer name from the Hugging Face model hub\n",
    "model_name = \"ayman56/llama2_finetuned_full_stackoverflow_test\"\n",
    "tokenizer_name  = \"ayman56/llama2_finetuned_full_stackoverflow_test\"\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (mistral)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1253d42-e3d9-44df-86a8-bb94e5030d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Prompts: 100%|██████████| 100/100 [1:20:28<00:00, 48.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Define a function to perform inference on a single question\n",
    "def infer_question(prompt):\n",
    "    # Run text generation pipeline with your model\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)\n",
    "    result = pipe(f\" {prompt} \")\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Load the test set from the CSV file\n",
    "test_set = pd.read_csv('test_set.csv')\n",
    "\n",
    "# Initialize lists to collect generated prompts, original questions, and original answers\n",
    "generated_prompts = []\n",
    "original_questions = []\n",
    "original_answers = []\n",
    "\n",
    "# Iterate over each row in the test set with tqdm to track progress\n",
    "for index, row in tqdm(test_set.iterrows(), total=len(test_set), desc=\"Generating Prompts\"):\n",
    "    question = row['question']\n",
    "    answer = row['answer']\n",
    "    \n",
    "    # Generate a prompt using the inference function\n",
    "    generated_prompt = infer_question(question)\n",
    "    \n",
    "    # Collect the generated prompt, original question, and original answer\n",
    "    generated_prompts.append(generated_prompt)\n",
    "    original_questions.append(question)\n",
    "    original_answers.append(answer)\n",
    "\n",
    "# Create a DataFrame to store the generated prompts, original questions, and original answers\n",
    "generated_prompts_dataset = pd.DataFrame({\n",
    "    'Generated_Prompt': generated_prompts,\n",
    "    'Original_Question': original_questions,\n",
    "    'Original_Answer': original_answers\n",
    "})\n",
    "\n",
    "# Save the generated prompts dataset to a CSV file\n",
    "generated_prompts_dataset.to_csv('generated_prompts_Llama2_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4f963-ee1a-4946-9043-8f9e36270390",
   "metadata": {},
   "source": [
    "# Inference of the base model Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c7129f-ae94-40e5-b596-cbcdf41aef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2be1b16510041e89c3a764933c5bf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# Specify the model and tokenizer name from the Hugging Face model hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer_name  = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (mistral)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091e2c3f-f922-4215-a0bb-bec989c23032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Prompts:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   1%|          | 1/100 [00:06<10:18,  6.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   2%|▏         | 2/100 [00:15<12:44,  7.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   3%|▎         | 3/100 [00:30<18:19, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   4%|▍         | 4/100 [00:46<20:45, 12.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   5%|▌         | 5/100 [00:58<20:27, 12.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   6%|▌         | 6/100 [01:14<21:44, 13.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   7%|▋         | 7/100 [01:26<20:17, 13.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   8%|▊         | 8/100 [01:42<21:49, 14.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:   9%|▉         | 9/100 [02:03<24:46, 16.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  10%|█         | 10/100 [02:14<21:52, 14.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  11%|█         | 11/100 [02:31<22:55, 15.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  12%|█▏        | 12/100 [02:48<23:06, 15.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  13%|█▎        | 13/100 [02:59<20:44, 14.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  14%|█▍        | 14/100 [03:12<19:54, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  15%|█▌        | 15/100 [03:30<21:32, 15.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  16%|█▌        | 16/100 [03:43<20:25, 14.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  17%|█▋        | 17/100 [03:49<16:36, 12.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  18%|█▊        | 18/100 [03:55<13:55, 10.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  19%|█▉        | 19/100 [04:10<15:43, 11.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  20%|██        | 20/100 [04:23<15:54, 11.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  21%|██        | 21/100 [04:37<16:30, 12.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  22%|██▏       | 22/100 [04:47<15:35, 11.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  23%|██▎       | 23/100 [05:04<17:07, 13.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  24%|██▍       | 24/100 [05:17<16:57, 13.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  25%|██▌       | 25/100 [05:19<12:26,  9.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  26%|██▌       | 26/100 [05:31<12:51, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  27%|██▋       | 27/100 [05:45<14:03, 11.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  28%|██▊       | 28/100 [06:00<14:56, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  29%|██▉       | 29/100 [06:07<13:05, 11.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  30%|███       | 30/100 [06:18<12:47, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  31%|███       | 31/100 [06:32<13:28, 11.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  32%|███▏      | 32/100 [06:42<12:40, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  33%|███▎      | 33/100 [06:53<12:42, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  34%|███▍      | 34/100 [07:15<15:43, 14.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  35%|███▌      | 35/100 [07:29<15:33, 14.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  36%|███▌      | 36/100 [07:41<14:24, 13.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  37%|███▋      | 37/100 [07:56<14:52, 14.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  38%|███▊      | 38/100 [08:12<15:06, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  39%|███▉      | 39/100 [08:31<16:10, 15.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  40%|████      | 40/100 [08:44<15:01, 15.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  41%|████      | 41/100 [08:57<14:14, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  42%|████▏     | 42/100 [09:16<15:22, 15.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  43%|████▎     | 43/100 [09:31<14:48, 15.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  44%|████▍     | 44/100 [09:40<12:37, 13.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  45%|████▌     | 45/100 [09:57<13:23, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  46%|████▌     | 46/100 [10:16<14:21, 15.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  47%|████▋     | 47/100 [10:27<12:39, 14.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  48%|████▊     | 48/100 [10:41<12:32, 14.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  49%|████▉     | 49/100 [10:54<11:48, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  50%|█████     | 50/100 [11:07<11:19, 13.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  51%|█████     | 51/100 [11:25<12:09, 14.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  52%|█████▏    | 52/100 [11:35<10:44, 13.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  53%|█████▎    | 53/100 [11:48<10:24, 13.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  54%|█████▍    | 54/100 [12:01<10:12, 13.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  55%|█████▌    | 55/100 [12:20<11:08, 14.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  56%|█████▌    | 56/100 [12:35<10:58, 14.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  57%|█████▋    | 57/100 [12:49<10:28, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  58%|█████▊    | 58/100 [13:09<11:23, 16.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  59%|█████▉    | 59/100 [13:18<09:39, 14.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  60%|██████    | 60/100 [13:28<08:38, 12.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  61%|██████    | 61/100 [13:47<09:30, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  62%|██████▏   | 62/100 [14:02<09:25, 14.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  63%|██████▎   | 63/100 [14:16<08:55, 14.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  64%|██████▍   | 64/100 [14:21<06:58, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  65%|██████▌   | 65/100 [14:31<06:32, 11.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  66%|██████▌   | 66/100 [14:45<06:53, 12.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  67%|██████▋   | 67/100 [14:56<06:26, 11.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  68%|██████▊   | 68/100 [15:09<06:25, 12.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  69%|██████▉   | 69/100 [15:15<05:21, 10.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  70%|███████   | 70/100 [15:32<06:09, 12.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  71%|███████   | 71/100 [15:45<06:05, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  72%|███████▏  | 72/100 [15:55<05:30, 11.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  73%|███████▎  | 73/100 [16:02<04:36, 10.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  74%|███████▍  | 74/100 [16:08<03:57,  9.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  75%|███████▌  | 75/100 [16:30<05:20, 12.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  76%|███████▌  | 76/100 [16:39<04:38, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  77%|███████▋  | 77/100 [16:53<04:42, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  78%|███████▊  | 78/100 [17:06<04:36, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  79%|███████▉  | 79/100 [17:17<04:14, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  80%|████████  | 80/100 [17:32<04:18, 12.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  81%|████████  | 81/100 [17:41<03:43, 11.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  82%|████████▏ | 82/100 [17:56<03:52, 12.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  83%|████████▎ | 83/100 [18:17<04:21, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  84%|████████▍ | 84/100 [18:29<03:46, 14.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  85%|████████▌ | 85/100 [18:38<03:10, 12.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  86%|████████▌ | 86/100 [18:54<03:13, 13.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  87%|████████▋ | 87/100 [19:10<03:06, 14.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  88%|████████▊ | 88/100 [19:15<02:19, 11.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  89%|████████▉ | 89/100 [19:16<01:33,  8.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  90%|█████████ | 90/100 [19:28<01:32,  9.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  91%|█████████ | 91/100 [19:37<01:24,  9.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  92%|█████████▏| 92/100 [19:41<01:01,  7.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  93%|█████████▎| 93/100 [19:53<01:02,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  94%|█████████▍| 94/100 [19:58<00:46,  7.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  95%|█████████▌| 95/100 [20:09<00:43,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  96%|█████████▌| 96/100 [20:19<00:37,  9.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  97%|█████████▋| 97/100 [20:34<00:32, 10.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  98%|█████████▊| 98/100 [20:43<00:20, 10.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts:  99%|█████████▉| 99/100 [20:57<00:11, 11.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Prompts: 100%|██████████| 100/100 [21:05<00:00, 12.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Define a function to perform inference on a single question\n",
    "def infer_question(prompt):\n",
    "    # Run text generation pipeline with your model\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "    result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Load the test set from the CSV file\n",
    "test_set = pd.read_csv('test_set.csv')\n",
    "\n",
    "# Initialize lists to collect generated prompts, original questions, and original answers\n",
    "generated_prompts = []\n",
    "original_questions = []\n",
    "original_answers = []\n",
    "\n",
    "# Iterate over each row in the test set with tqdm to track progress\n",
    "for index, row in tqdm(test_set.iterrows(), total=len(test_set), desc=\"Generating Prompts\"):\n",
    "    question = row['question']\n",
    "    answer = row['answer']\n",
    "    \n",
    "    # Generate a prompt using the inference function\n",
    "    generated_prompt = infer_question(question)\n",
    "    \n",
    "    # Collect the generated prompt, original question, and original answer\n",
    "    generated_prompts.append(generated_prompt)\n",
    "    original_questions.append(question)\n",
    "    original_answers.append(answer)\n",
    "\n",
    "# Create a DataFrame to store the generated prompts, original questions, and original answers\n",
    "generated_prompts_dataset = pd.DataFrame({\n",
    "    'Generated_Prompt': generated_prompts,\n",
    "    'Original_Question': original_questions,\n",
    "    'Original_Answer': original_answers\n",
    "})\n",
    "\n",
    "# Save the generated prompts dataset to a CSV file\n",
    "generated_prompts_dataset.to_csv('generated_prompts_baseMistral_7b.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46008e41",
   "metadata": {},
   "source": [
    "All the generated scirpts are saved and will be used for evaluation in Evaluation notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00d1fc2a86e8423e9ae9962627843574": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "012ffd0a4ca6458e97831decc2b3746e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "024193fec8264ff99786772dafcb5e00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "031e9e17491d4750b056d0e0db3789de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "07c52f6532874d94920c7a4f0c1a41c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0cce0eb1736b46f3b91f0080522a7b71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0ec6be0bee59493bb390f5b5b5d969ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0fff9bdb307e431a9c34b836fa698d28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "122a6e85a0f74c1bbe32a2d28abeeeab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1474c7bd39534f5da30668772ad8b17f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3c68d678e6bf4259af6c01ba9cd665f4",
       "style": "IPY_MODEL_07c52f6532874d94920c7a4f0c1a41c7",
       "value": "model.safetensors.index.json: 100%"
      }
     },
     "1613931e169f48ecb96462e0fcd2d23e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "16649db75416496dab9f76faec6b3a8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_00d1fc2a86e8423e9ae9962627843574",
       "style": "IPY_MODEL_2238b37060114d7e98c469a95d4aaa3c",
       "value": " 3/3 [01:40&lt;00:00, 32.85s/it]"
      }
     },
     "171dae0dd398437384a7db78a4f66a31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "185adf440d33454781384b9f01448727": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "19bd3e22063d4700a383c5eb85cbf5e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1e06fad2cf084566a057cee937536e2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1e64578cd1b44c128e605f97fca06986": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1e9f1e7a4efe4365bda1cd54dd3cddf3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1ec10fe3834b4da7bb1fe978e81b1e4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ec39c73b658646a487648fe872eaca0b",
       "style": "IPY_MODEL_f8230f522a0b429085fd4f599151c769",
       "value": " 4.54G/4.54G [00:28&lt;00:00, 244MB/s]"
      }
     },
     "1f6eb8a269d3484d96b8e2a6978b4c22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ae6006f5f42b475ea4908e111a204b93",
       "style": "IPY_MODEL_1613931e169f48ecb96462e0fcd2d23e",
       "value": " 5.00G/5.00G [00:36&lt;00:00, 137MB/s]"
      }
     },
     "2238b37060114d7e98c469a95d4aaa3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2266e6effe444f37b8b13e63d3b7f06a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2763f2caf2e14f129630e1e2dfc7b6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9c3ca5d206bf46c9a18b8cb781676845",
       "style": "IPY_MODEL_024193fec8264ff99786772dafcb5e00",
       "value": "generation_config.json: 100%"
      }
     },
     "2d798a87b6294095bfcc5577fb9115ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2d8ffb7e64dd4d31afcd0286aa5d1e95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ebd075065fc4e16a316e3968aa4bc30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5fc97687a7654837b856231fc9f43583",
        "IPY_MODEL_93149f9b64be4641b341fbe54120c68a",
        "IPY_MODEL_1ec10fe3834b4da7bb1fe978e81b1e4c"
       ],
       "layout": "IPY_MODEL_8704fa3806a14467a59f6b9f7659c7f7"
      }
     },
     "301f138bd7e641e99a8d9823300abfa2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7d4e28f164e841a98f80e1e7b459a90e",
       "style": "IPY_MODEL_dfbe29faacbe41b48bf52ac75d0a7222",
       "value": " 3/3 [00:06&lt;00:00,  2.13s/it]"
      }
     },
     "30c1641dd0504614b04d7476c871f813": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30f193b64e4f431b991f7aa5d62f64de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7894c6619b784fb3a77728bb253284fc",
       "max": 596,
       "style": "IPY_MODEL_ed9428db17b448a5a0f2e55d887de4c4",
       "value": 596
      }
     },
     "339dbbf9c45f4e858a95b4ab9f970bdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3735adfd08e843b0a4ba14abdb99a2ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_de18927c9f5a43f1994165701796fe99",
       "style": "IPY_MODEL_3ce44195d4bb421bb3379bfb4a9f562a",
       "value": "Downloading shards: 100%"
      }
     },
     "37b9d1bd37364e7782b85f19d52bfd53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b4339b620e074601ba53687f0ac3ee2a",
       "max": 3,
       "style": "IPY_MODEL_390abbdf422f4ba3b20af9e27fe6c537",
       "value": 3
      }
     },
     "3880e7d099664027b14e61e7dd6e7d55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "38d5ed581ba74c63b9619273f875e887": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2d8ffb7e64dd4d31afcd0286aa5d1e95",
       "style": "IPY_MODEL_b62bdd2317864c7ca55c89c207bb0348",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "390abbdf422f4ba3b20af9e27fe6c537": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3c68d678e6bf4259af6c01ba9cd665f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ce44195d4bb421bb3379bfb4a9f562a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3fbbf315097b4096a92f1bf1c843fe69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4093408518f046d5902fa127b74024e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f1082b59f66d4cae829c9b357d687446",
       "style": "IPY_MODEL_6af6406dc2874a88a45e7ed0fca7284c",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "431b7f4c87014450817e1c3bf258ff5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "431f85adc1ce412c9549a97774f51f0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "44df4570854a41d489f729cef609a993": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "47bcd7fa6e6b4c2b94ad6623efc10c0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "48161fc20178408aa8660638cd056938": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_031e9e17491d4750b056d0e0db3789de",
       "style": "IPY_MODEL_756ef44af04d414c8424051266f76457",
       "value": "model-00002-of-00003.safetensors: 100%"
      }
     },
     "4a44333e67984e6ea0b6852036300e09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7c0a206c596a4a499cdf2ff68e772015",
       "style": "IPY_MODEL_ea3c754239e94a178762b623cbee02b2",
       "value": " 493k/493k [00:00&lt;00:00, 24.3MB/s]"
      }
     },
     "536f98fca9f942cebacb9db40248af30": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "54b40070af4449cc8ef8738cf7f48e0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5678c9bed4f44933a66bdda4cedc29d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5c2e8a308ef6424c9225a5c7daa9e54b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3735adfd08e843b0a4ba14abdb99a2ac",
        "IPY_MODEL_ec9c1b62ef464327b5f3881d86693a42",
        "IPY_MODEL_16649db75416496dab9f76faec6b3a8a"
       ],
       "layout": "IPY_MODEL_8f157210c08240ebaf0ae3d7f57326f0"
      }
     },
     "5c9e33bd5fbf46e89078ea9c5e9e8de1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5d314f6c2b544ba4ae925f37b20b4505": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_536f98fca9f942cebacb9db40248af30",
       "style": "IPY_MODEL_19bd3e22063d4700a383c5eb85cbf5e6",
       "value": "config.json: 100%"
      }
     },
     "5fc97687a7654837b856231fc9f43583": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1e9f1e7a4efe4365bda1cd54dd3cddf3",
       "style": "IPY_MODEL_de4e87b1eec84307ad4d797a5036d313",
       "value": "model-00003-of-00003.safetensors: 100%"
      }
     },
     "667c6f5d828b4e3aa27b94b11211835d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "696ce2cdce544e70baed6e7f4a321934": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6af6406dc2874a88a45e7ed0fca7284c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b8e4acc8f6e444e80a7d1cc7bae85eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_86973625aea744caa26812688af83fd6",
       "max": 72,
       "style": "IPY_MODEL_d38156730be34d5e9943ab98a0f308b0",
       "value": 72
      }
     },
     "6cc23f1966ab4e15b9721e42f8772002": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6e56c5970e9a45c5b11e8b0a4bcc902f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_012ffd0a4ca6458e97831decc2b3746e",
       "max": 1795303,
       "style": "IPY_MODEL_ae37d674cef747e58ef26d68a83cdb45",
       "value": 1795303
      }
     },
     "70931e22768a47349f8c951fac987c46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "755f3aa634b449108a797a925b895602": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_30c1641dd0504614b04d7476c871f813",
       "style": "IPY_MODEL_122a6e85a0f74c1bbe32a2d28abeeeab",
       "value": " 72.0/72.0 [00:00&lt;00:00, 4.14kB/s]"
      }
     },
     "756ef44af04d414c8424051266f76457": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "75dfeab6be954ed192d08653d8f418f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "75e00a6d8c7745088a32ce521cc67697": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_78fbc6fc9b0849a9ab63b549da7e122d",
       "style": "IPY_MODEL_c560f9a67e5749418eb4183957197bfb",
       "value": " 3/3 [00:06&lt;00:00,  2.25s/it]"
      }
     },
     "7894c6619b784fb3a77728bb253284fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "78fbc6fc9b0849a9ab63b549da7e122d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7c0a206c596a4a499cdf2ff68e772015": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7d02e5d8c08348c7979cb8a591d6b5ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7d4e28f164e841a98f80e1e7b459a90e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "80d1ed189acb4a2cb368f1eee04f676e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_70931e22768a47349f8c951fac987c46",
       "max": 111,
       "style": "IPY_MODEL_9f794ea3334d448aad571d7fb06dba20",
       "value": 111
      }
     },
     "82dc7da7c3f0433196a5271f2b7e4442": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5678c9bed4f44933a66bdda4cedc29d1",
       "style": "IPY_MODEL_a8506aca5a194c2b86aa5880d4dc49c4",
       "value": " 25.1k/25.1k [00:00&lt;00:00, 997kB/s]"
      }
     },
     "86973625aea744caa26812688af83fd6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8704fa3806a14467a59f6b9f7659c7f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c45d47895f144e1b492fde65925ae69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_ffa7537d701548bfabeec7f7937f07d4",
       "max": 3,
       "style": "IPY_MODEL_696ce2cdce544e70baed6e7f4a321934",
       "value": 3
      }
     },
     "8d3abe504164434eb1391e08c758b1d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_48161fc20178408aa8660638cd056938",
        "IPY_MODEL_a902cbbf4f404c438a036e5693da1907",
        "IPY_MODEL_1f6eb8a269d3484d96b8e2a6978b4c22"
       ],
       "layout": "IPY_MODEL_1e64578cd1b44c128e605f97fca06986"
      }
     },
     "8d4b08bb6e4b4b94b3db7a4f599c0ee6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ee0635f175b544b5ad04a4841517cfde",
       "style": "IPY_MODEL_ea20c1dcbd3540c197677559517d45dd",
       "value": " 1.80M/1.80M [00:00&lt;00:00, 4.85MB/s]"
      }
     },
     "8e1046b3f295438b8faf7b666fb1174f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4093408518f046d5902fa127b74024e2",
        "IPY_MODEL_9984076fa511430fad36716ce77f188a",
        "IPY_MODEL_b169fa09e1544cf291a3466bbc6861ef"
       ],
       "layout": "IPY_MODEL_44df4570854a41d489f729cef609a993"
      }
     },
     "8f157210c08240ebaf0ae3d7f57326f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "93149f9b64be4641b341fbe54120c68a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b1aed204cfd245f0bd3457446c01e67e",
       "max": 4540516344,
       "style": "IPY_MODEL_431f85adc1ce412c9549a97774f51f0d",
       "value": 4540516344
      }
     },
     "95b93ae2f49e4f4c8c9182b8cb61dc7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f1478ca0bfa743e3be531bf57df1d43b",
       "style": "IPY_MODEL_0fff9bdb307e431a9c34b836fa698d28",
       "value": " 596/596 [00:00&lt;00:00, 20.3kB/s]"
      }
     },
     "96d07ac010aa4b13ab9cee861eae6879": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a92197a6818d4f039ff168261096ab8f",
       "style": "IPY_MODEL_99157b9d2a3948c1847b2c5aa2d2c6cf",
       "value": " 4.94G/4.94G [00:34&lt;00:00, 199MB/s]"
      }
     },
     "9826ecad57134564a6998998e3adc597": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ac1a2adb209f49b7925edb954003c9eb",
        "IPY_MODEL_6b8e4acc8f6e444e80a7d1cc7bae85eb",
        "IPY_MODEL_755f3aa634b449108a797a925b895602"
       ],
       "layout": "IPY_MODEL_f76228ee489a48abadbffec89932ad63"
      }
     },
     "99157b9d2a3948c1847b2c5aa2d2c6cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9984076fa511430fad36716ce77f188a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_dbe839ad95a34a7dbbfac4c15f3be674",
       "max": 1460,
       "style": "IPY_MODEL_2d798a87b6294095bfcc5577fb9115ea",
       "value": 1460
      }
     },
     "9c3ca5d206bf46c9a18b8cb781676845": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9d446cba4f3f492ebc5ee9357cdc0956": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9f2f9c7e34924a25bb1a0117ba284ff9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9f794ea3334d448aad571d7fb06dba20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a2b507b512bf4620825eba1bc8196d99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2763f2caf2e14f129630e1e2dfc7b6bc",
        "IPY_MODEL_80d1ed189acb4a2cb368f1eee04f676e",
        "IPY_MODEL_bc374235e3334d908d3df7ac55deb20e"
       ],
       "layout": "IPY_MODEL_667c6f5d828b4e3aa27b94b11211835d"
      }
     },
     "a5d8513592ce46879e6d51352a3ce2cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a8506aca5a194c2b86aa5880d4dc49c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a902cbbf4f404c438a036e5693da1907": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_339dbbf9c45f4e858a95b4ab9f970bdb",
       "max": 4999819336,
       "style": "IPY_MODEL_6cc23f1966ab4e15b9721e42f8772002",
       "value": 4999819336
      }
     },
     "a92197a6818d4f039ff168261096ab8f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ac1154e47318481b9b280d1f49cb351a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ac1a2adb209f49b7925edb954003c9eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_431b7f4c87014450817e1c3bf258ff5e",
       "style": "IPY_MODEL_9f2f9c7e34924a25bb1a0117ba284ff9",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "acf0969819b8470899ebc0de43815da1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ad13aeb876394cde9b1df80307267694": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_acf0969819b8470899ebc0de43815da1",
       "style": "IPY_MODEL_e74f436dfae14118a3dce36e5bd9635f",
       "value": "tokenizer.json: 100%"
      }
     },
     "ae37d674cef747e58ef26d68a83cdb45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ae6006f5f42b475ea4908e111a204b93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b169fa09e1544cf291a3466bbc6861ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_171dae0dd398437384a7db78a4f66a31",
       "style": "IPY_MODEL_9d446cba4f3f492ebc5ee9357cdc0956",
       "value": " 1.46k/1.46k [00:00&lt;00:00, 83.8kB/s]"
      }
     },
     "b1aed204cfd245f0bd3457446c01e67e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b1d7e38d50c54c47881a59c7645711b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2266e6effe444f37b8b13e63d3b7f06a",
       "style": "IPY_MODEL_b4fd2d6a4f7545f887708c2027c3b0b1",
       "value": "model-00001-of-00003.safetensors: 100%"
      }
     },
     "b4339b620e074601ba53687f0ac3ee2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b4fd2d6a4f7545f887708c2027c3b0b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b62bdd2317864c7ca55c89c207bb0348": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bc374235e3334d908d3df7ac55deb20e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d9dc3fdeb890471b95ba448ba87aa1a1",
       "style": "IPY_MODEL_75dfeab6be954ed192d08653d8f418f9",
       "value": " 111/111 [00:00&lt;00:00, 1.60kB/s]"
      }
     },
     "c560f9a67e5749418eb4183957197bfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cec251d9fdb84071907ee60d4bd0a28e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f89fe1e05d2844fab7f7215e281e56ec",
        "IPY_MODEL_daf5f9e607154f4ab79495c6b91a9eff",
        "IPY_MODEL_4a44333e67984e6ea0b6852036300e09"
       ],
       "layout": "IPY_MODEL_e0f89b938fdd44f8aba11bd7d01001ca"
      }
     },
     "d2be1b16510041e89c3a764933c5bf2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_38d5ed581ba74c63b9619273f875e887",
        "IPY_MODEL_8c45d47895f144e1b492fde65925ae69",
        "IPY_MODEL_301f138bd7e641e99a8d9823300abfa2"
       ],
       "layout": "IPY_MODEL_54b40070af4449cc8ef8738cf7f48e0d"
      }
     },
     "d38156730be34d5e9943ab98a0f308b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d4290ed93c8b4492a229007a09014150": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e3fc6a22126740c2adedf067c6aabf17",
        "IPY_MODEL_37b9d1bd37364e7782b85f19d52bfd53",
        "IPY_MODEL_75e00a6d8c7745088a32ce521cc67697"
       ],
       "layout": "IPY_MODEL_185adf440d33454781384b9f01448727"
      }
     },
     "d8ad821a4449487885752f2a5c8e3b77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d91d1085b2af4100879b8b5a3bee1e19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ad13aeb876394cde9b1df80307267694",
        "IPY_MODEL_6e56c5970e9a45c5b11e8b0a4bcc902f",
        "IPY_MODEL_8d4b08bb6e4b4b94b3db7a4f599c0ee6"
       ],
       "layout": "IPY_MODEL_1e06fad2cf084566a057cee937536e2e"
      }
     },
     "d9dc3fdeb890471b95ba448ba87aa1a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "da16e66cb4d1450cbdc12d035521e8b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_e9fd521638ef46b89f384af4ba9e70b3",
       "max": 25125,
       "style": "IPY_MODEL_7d02e5d8c08348c7979cb8a591d6b5ce",
       "value": 25125
      }
     },
     "daf5f9e607154f4ab79495c6b91a9eff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_3fbbf315097b4096a92f1bf1c843fe69",
       "max": 493443,
       "style": "IPY_MODEL_ac1154e47318481b9b280d1f49cb351a",
       "value": 493443
      }
     },
     "dbe839ad95a34a7dbbfac4c15f3be674": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dd39c73970454d9384efde8df317ea3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5d314f6c2b544ba4ae925f37b20b4505",
        "IPY_MODEL_30f193b64e4f431b991f7aa5d62f64de",
        "IPY_MODEL_95b93ae2f49e4f4c8c9182b8cb61dc7d"
       ],
       "layout": "IPY_MODEL_f073824e7154482f822930274cf6eecc"
      }
     },
     "de18927c9f5a43f1994165701796fe99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "de4e87b1eec84307ad4d797a5036d313": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dfbe29faacbe41b48bf52ac75d0a7222": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e0f89b938fdd44f8aba11bd7d01001ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e3fc6a22126740c2adedf067c6aabf17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a5d8513592ce46879e6d51352a3ce2cd",
       "style": "IPY_MODEL_5c9e33bd5fbf46e89078ea9c5e9e8de1",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "e74f436dfae14118a3dce36e5bd9635f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e9fd521638ef46b89f384af4ba9e70b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ea20c1dcbd3540c197677559517d45dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ea3c754239e94a178762b623cbee02b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eb6d9191af7148dd935f4dfc1301324d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec39c73b658646a487648fe872eaca0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec9c1b62ef464327b5f3881d86693a42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_eb6d9191af7148dd935f4dfc1301324d",
       "max": 3,
       "style": "IPY_MODEL_0cce0eb1736b46f3b91f0080522a7b71",
       "value": 3
      }
     },
     "ed9428db17b448a5a0f2e55d887de4c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ee0635f175b544b5ad04a4841517cfde": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f073824e7154482f822930274cf6eecc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f1082b59f66d4cae829c9b357d687446": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f1478ca0bfa743e3be531bf57df1d43b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f227a7b419d0433982acad7bcfb85144": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1474c7bd39534f5da30668772ad8b17f",
        "IPY_MODEL_da16e66cb4d1450cbdc12d035521e8b2",
        "IPY_MODEL_82dc7da7c3f0433196a5271f2b7e4442"
       ],
       "layout": "IPY_MODEL_47bcd7fa6e6b4c2b94ad6623efc10c0d"
      }
     },
     "f6a06fc2cacd46428997c617b514b0c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b1d7e38d50c54c47881a59c7645711b1",
        "IPY_MODEL_fe8b9b221ad647e5afb61010002989de",
        "IPY_MODEL_96d07ac010aa4b13ab9cee861eae6879"
       ],
       "layout": "IPY_MODEL_0ec6be0bee59493bb390f5b5b5d969ba"
      }
     },
     "f76228ee489a48abadbffec89932ad63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f8230f522a0b429085fd4f599151c769": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f89fe1e05d2844fab7f7215e281e56ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3880e7d099664027b14e61e7dd6e7d55",
       "style": "IPY_MODEL_d8ad821a4449487885752f2a5c8e3b77",
       "value": "tokenizer.model: 100%"
      }
     },
     "fa535c9190ae44b2a2e7122c6c332e90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fdb8a86fe99542db8bab1bec37eaff9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "fe8b9b221ad647e5afb61010002989de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_fa535c9190ae44b2a2e7122c6c332e90",
       "max": 4943162336,
       "style": "IPY_MODEL_fdb8a86fe99542db8bab1bec37eaff9e",
       "value": 4943162336
      }
     },
     "ffa7537d701548bfabeec7f7937f07d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
