{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b42da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import InferenceClient, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64511b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ea2e92e13b430abd97f2f85fa7fe81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install huggingface_hub datasets pandas tqdm -q\n",
    "\n",
    "# Set up tqdm for progress bars\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Log in to Hugging Face Hub if needed\n",
    "notebook_login()\n",
    "\n",
    "# Initializing the InferenceClient with the LLM model\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm_client = InferenceClient(model=repo_id, timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add1fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: \n",
      "\n",
      "I’m going to the gym, then I’m going to the grocery store,\n"
     ]
    }
   ],
   "source": [
    "# Testing the LLM client\n",
    "response = llm_client.text_generation(prompt=\"What are your plans for today?\", max_new_tokens=20)\n",
    "print(\"LLM Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59b3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for LLM Evaluation\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "You will be given an Instruction (for the generation of an SAS code) and Generated Solution (SAS code) couple.\n",
    "Your task is to provide a 'total rating' scoring how well the Generated Solution answers the user concerns expressed in the Instruction.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the Generated Solution is not helpful at all, and 5 means that the Generated Solution completely and helpfully addresses the Instruction.\n",
    "\n",
    "\n",
    "\n",
    "Here is the scale you should use to build your answer:\n",
    "1: The Generated Solution is terrible: completely irrelevant to the Instruction given, very partial, or generates an error when run\n",
    "2: The Generated Solution is mostly not helpful: misses some key aspects of the Instruction, or if there is a syntax error\n",
    "3: The Generated Solution is mostly helpful: provides support, but still could be improved\n",
    "4: The Generated Solution is excellent: relevant, direct, detailed, and addresses all the concerns raised in the Instruction\n",
    "5: The Generated Solution exceeds expectations: relevant, direct, detailed, addresses all the concerns raised in the Instruction, and demonstrates exceptional innovation, optimization, or elegance.\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the instruction and generated solution.\n",
    "\n",
    "Instruction: {instruction}\n",
    "Generated Solution: {generated_solution}\n",
    "\n",
    "Provide your feedback.\n",
    "Feedback:::\n",
    "Evaluation: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3770ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judge_score(answer: str, split_str: str = \"Total rating:\") -> int:\n",
    "    try:\n",
    "        if split_str in answer:\n",
    "            rating = answer.split(split_str)[1]\n",
    "        else:\n",
    "            rating = answer\n",
    "        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n",
    "        return float(digit_groups[0])\n",
    "    except Exception as e:\n",
    "        print(\"Error in extract_judge_score:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca1036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate generated solutions using the LLM\n",
    "def evaluate_with_llm(instruction, generated_solution):\n",
    "    try:\n",
    "        response = llm_client.text_generation(\n",
    "            prompt=JUDGE_PROMPT.format(instruction=instruction, generated_solution=generated_solution),\n",
    "            max_new_tokens=1000,\n",
    "        )\n",
    "        print(\"LLM Response:\", response)  # Debugging line\n",
    "        score = extract_judge_score(response)\n",
    "        print(\"Extracted Score:\", score)  # Debugging line\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during LLM generation:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed61f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: \n",
      "The generated solution is excellent. It is relevant, direct, detailed, and addresses all the concerns raised in the instruction. The code generates a random number between 1 and 10, and the put statement prints the result.\n",
      "Total rating: 4\n",
      "Extracted Score: 4.0\n",
      "LLM Evaluation Score: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "instruction = \"Write an SAS code that generates a random number between 1 and 10\"\n",
    "generated_solution = \"\"\"\n",
    "\n",
    "data _null_;\n",
    "    random_number = ceil(rand('uniform') * 10);\n",
    "    put \"Random number between 1 and 10: \" random_number;\n",
    "run;\n",
    "\n",
    "\"\"\"\n",
    "llm_evaluation_score = evaluate_with_llm(instruction, generated_solution)\n",
    "print(\"LLM Evaluation Score:\", llm_evaluation_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d62ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: \n",
      "The Generated Solution is terrible: completely irrelevant to the Instruction given, very partial, or generates an error when run.\n",
      "The Generated Solution is written in Python, not SAS.\n",
      "Total rating: 1\n",
      "Extracted Score: 1.0\n",
      "LLM Evaluation Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "instruction = \"Write an SAS code that generates a random number between 1 and 10\"\n",
    "generated_solution = \"\"\"\n",
    "import random\n",
    "random_number = arandom.randit(1, 10)\n",
    "print(\"Random number between 1 and 10:\", random_number)\n",
    "\"\"\"\n",
    "llm_evaluation_score = evaluate_with_llm(instruction, generated_solution)\n",
    "print(\"LLM Evaluation Score:\", llm_evaluation_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a568dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: \n",
      "The Generated Solution is mostly not helpful. It does not describe an apple, but rather compares it to a car and a banana.\n",
      "Total rating: 2\n",
      "Extracted Score: 2.0\n",
      "LLM Evaluation Score: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "instruction = \"Give a sentence in which you describe an apple.\"\n",
    "generated_solution = \"An apple is like a car but also like a banana.\"\n",
    "llm_evaluation_score = evaluate_with_llm(instruction, generated_solution)\n",
    "print(\"LLM Evaluation Score:\", llm_evaluation_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b40a0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate generated solutions using the LLM and return the mean score\n",
    "def evaluate_with_llm2(instructions, generated_solutions):\n",
    "    scores = []\n",
    "    for instruction, generated_solution in zip(instructions, generated_solutions):\n",
    "        try:\n",
    "            response = llm_client.text_generation(\n",
    "                prompt=JUDGE_PROMPT.format(instruction=instruction, generated_solution=generated_solution),\n",
    "                max_new_tokens=1000,\n",
    "            )\n",
    "            score = extract_judge_score(response)\n",
    "            if score is not None:\n",
    "                scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred during LLM generation:\", e)\n",
    "    if scores:\n",
    "        return sum(scores) / len(scores)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdec03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
