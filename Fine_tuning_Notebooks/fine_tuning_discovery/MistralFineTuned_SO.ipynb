{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1224787d-0c6e-4d0b-8c43-a290605320e7",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e238df-4a49-4160-877a-9fc6f5ba5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/zealous_easley/.local/lib/python3.8/site-packages (0.1.99)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# we use the latest version of transformers, peft, and accelerate\n",
    "!pip install -q accelerate peft transformers\n",
    "\n",
    "# install bitsandbytes for quantization\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "# install trl for the SFT library\n",
    "!pip install -q trl\n",
    "\n",
    "# we need sentencepiece for the llama2 slow tokenizer\n",
    "!pip install sentencepiece\n",
    "\n",
    "# we need einops, used by falcon-7b, llama-2 etc\n",
    "# einops (einsteinops) is used to simplify tensorops by making them readable\n",
    "!pip install -q -U einops\n",
    "\n",
    "# we need to install datasets for our training dataset\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a44e3-f953-4991-9803-17e36888abe6",
   "metadata": {},
   "source": [
    "# Loading model Mistral 7b and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f90b76c-cc3b-425a-97b1-b5d3b63db6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that we want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# The instruction dataset to use found on HuggingFace\n",
    "dataset_name = \"KonradSzafer/stackoverflow_python_preprocessed\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Mistral-7B-Stackoverflow\"\n",
    "output_dir = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6573504-3d44-49b1-8a1c-0f8349231dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 08:22:47.377639: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see solightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-06 08:22:47.426613: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4a6ef3dbf74d1c8621abe738a14e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (llama2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19402d77-f9e8-4ec7-a4b2-7e29e2be2fc9",
   "metadata": {},
   "source": [
    "# Testing the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be93cca-f7cd-42ec-9a28-b72f2531b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] If you're writing a library, or an app, where do the unit test files go? It's nice to separate the test files from the main app code, but it's awkward to put them into a 'tests' subdirectory inside of the app root directory, because it makes it harder to import the modules that you'll be testing. Is there a best practice here? [/INST] In software development, it's common to keep unit tests separate from the main application code for organization, maintainability, and testability reasons. However, you're right that putting tests in a 'tests' subdirectory within the app root directory can make importing the modules being tested more difficult.\n",
      "\n",
      "Instead, many developers prefer to place the unit tests in a separate directory at the same level as the application code. This is often referred to as the \"test suite\" or \"test folder.\" This approach makes it easier to import the modules being\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "\n",
    "prompt = \"If you're writing a library, or an app, where do the unit test files go? It's nice to separate the test files from the main app code, but it's awkward to put them into a 'tests' subdirectory inside of the app root directory, because it makes it harder to import the modules that you'll be testing. Is there a best practice here?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a9cd2-6d11-4b5f-aeb5-f7d1598a6456",
   "metadata": {},
   "source": [
    "# Fine tuning the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f882dce-5f31-458a-b04c-01460b88dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Create the new 'text' column by concatenating the formatted text\n",
    "df['text'] = '<s>[INST] ' + df['question'] + ' [/INST] (' + df['answer'] + ') </s>'\n",
    "\n",
    "# Keep only the 'text' column in the new dataset\n",
    "new_df = df[['text']]\n",
    "# Convert DataFrame to dataset\n",
    "new_df = Dataset.from_pandas(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3198930-d39c-4e7b-812a-1a556b4badca",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b23454-2f3d-405c-85c1-ac4436fe9c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laughing_bhabha/.local/lib/python3.8/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d7753274864245a19d9100de389ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4308, 'grad_norm': 1.2479355335235596, 'learning_rate': 0.00016129032258064516, 'epoch': 0.02}\n",
      "{'loss': 2.544, 'grad_norm': 0.7114229798316956, 'learning_rate': 0.0003225806451612903, 'epoch': 0.05}\n",
      "{'loss': 2.1858, 'grad_norm': 0.6336128115653992, 'learning_rate': 0.0004838709677419355, 'epoch': 0.07}\n",
      "{'loss': 2.1411, 'grad_norm': 0.5810688734054565, 'learning_rate': 0.0006451612903225806, 'epoch': 0.1}\n",
      "{'loss': 1.936, 'grad_norm': 0.9818006157875061, 'learning_rate': 0.0008064516129032258, 'epoch': 0.12}\n",
      "{'loss': 1.9895, 'grad_norm': 0.5644474029541016, 'learning_rate': 0.000967741935483871, 'epoch': 0.15}\n",
      "{'loss': 1.8008, 'grad_norm': 0.6513486504554749, 'learning_rate': 0.0011290322580645162, 'epoch': 0.17}\n",
      "{'loss': 1.8841, 'grad_norm': 0.607361376285553, 'learning_rate': 0.0012903225806451613, 'epoch': 0.19}\n",
      "{'loss': 1.8131, 'grad_norm': 2.50314998626709, 'learning_rate': 0.0014516129032258066, 'epoch': 0.22}\n",
      "{'loss': 1.7884, 'grad_norm': 0.9189577102661133, 'learning_rate': 0.0016129032258064516, 'epoch': 0.24}\n",
      "{'loss': 1.7717, 'grad_norm': 0.9791968464851379, 'learning_rate': 0.0017741935483870969, 'epoch': 0.27}\n",
      "{'loss': 1.7631, 'grad_norm': 1.086719274520874, 'learning_rate': 0.001935483870967742, 'epoch': 0.29}\n",
      "{'loss': 1.8135, 'grad_norm': 1.066174864768982, 'learning_rate': 0.0019999888744757143, 'epoch': 0.32}\n",
      "{'loss': 1.7261, 'grad_norm': 1.5316672325134277, 'learning_rate': 0.0019999208860571253, 'epoch': 0.34}\n",
      "{'loss': 1.81, 'grad_norm': 1.7289655208587646, 'learning_rate': 0.001999791094263926, 'epoch': 0.36}\n",
      "{'loss': 1.8812, 'grad_norm': 1.4915088415145874, 'learning_rate': 0.0019995995071183216, 'epoch': 0.39}\n",
      "{'loss': 1.8217, 'grad_norm': 1.2727729082107544, 'learning_rate': 0.001999346136461981, 'epoch': 0.41}\n",
      "{'loss': 1.7896, 'grad_norm': 1.2968720197677612, 'learning_rate': 0.0019990309979553045, 'epoch': 0.44}\n",
      "{'loss': 1.7264, 'grad_norm': 1.6653062105178833, 'learning_rate': 0.0019986541110764565, 'epoch': 0.46}\n",
      "{'loss': 1.826, 'grad_norm': 1.8565713167190552, 'learning_rate': 0.001998215499120161, 'epoch': 0.49}\n",
      "{'loss': 1.8513, 'grad_norm': 1.8229762315750122, 'learning_rate': 0.0019977151891962617, 'epoch': 0.51}\n",
      "{'loss': 1.8161, 'grad_norm': 2.0064942836761475, 'learning_rate': 0.0019971532122280465, 'epoch': 0.53}\n",
      "{'loss': 1.8654, 'grad_norm': 1.6509541273117065, 'learning_rate': 0.0019965296029503367, 'epoch': 0.56}\n",
      "{'loss': 1.799, 'grad_norm': 1.9783000946044922, 'learning_rate': 0.0019958443999073395, 'epoch': 0.58}\n",
      "{'loss': 1.7234, 'grad_norm': 1.9440255165100098, 'learning_rate': 0.001995097645450266, 'epoch': 0.61}\n",
      "{'loss': 1.8521, 'grad_norm': 1.6958237886428833, 'learning_rate': 0.001994289385734713, 'epoch': 0.63}\n",
      "{'loss': 1.6899, 'grad_norm': 1.8543699979782104, 'learning_rate': 0.001993419670717811, 'epoch': 0.66}\n",
      "{'loss': 1.7384, 'grad_norm': 1.7242931127548218, 'learning_rate': 0.001992488554155135, 'epoch': 0.68}\n",
      "{'loss': 1.7686, 'grad_norm': 1.5683164596557617, 'learning_rate': 0.0019914960935973848, 'epoch': 0.7}\n",
      "{'loss': 1.801, 'grad_norm': 2.05795955657959, 'learning_rate': 0.001990442350386825, 'epoch': 0.73}\n",
      "{'loss': 1.8174, 'grad_norm': 1.8735415935516357, 'learning_rate': 0.0019893273896534934, 'epoch': 0.75}\n",
      "{'loss': 1.7845, 'grad_norm': 2.328953742980957, 'learning_rate': 0.0019881512803111797, 'epoch': 0.78}\n",
      "{'loss': 1.8204, 'grad_norm': 2.341308355331421, 'learning_rate': 0.00198691409505316, 'epoch': 0.8}\n",
      "{'loss': 1.772, 'grad_norm': 1.6922942399978638, 'learning_rate': 0.0019856159103477087, 'epoch': 0.83}\n",
      "{'loss': 1.58, 'grad_norm': 1.8778729438781738, 'learning_rate': 0.0019842568064333686, 'epoch': 0.85}\n",
      "{'loss': 1.7345, 'grad_norm': 2.545304775238037, 'learning_rate': 0.001982836867313995, 'epoch': 0.87}\n",
      "{'loss': 1.6453, 'grad_norm': 2.1942245960235596, 'learning_rate': 0.0019813561807535598, 'epoch': 0.9}\n",
      "{'loss': 1.6166, 'grad_norm': 1.8744460344314575, 'learning_rate': 0.0019798148382707296, 'epoch': 0.92}\n",
      "{'loss': 1.6896, 'grad_norm': 2.276120185852051, 'learning_rate': 0.0019782129351332083, 'epoch': 0.95}\n",
      "{'loss': 1.7391, 'grad_norm': 2.5020413398742676, 'learning_rate': 0.001976550570351849, 'epoch': 0.97}\n",
      "{'loss': 1.6886, 'grad_norm': 1.8242322206497192, 'learning_rate': 0.0019748278466745345, 'epoch': 1.0}\n",
      "{'loss': 1.4311, 'grad_norm': 2.525756597518921, 'learning_rate': 0.0019730448705798237, 'epoch': 1.02}\n",
      "{'loss': 1.504, 'grad_norm': 2.209148645401001, 'learning_rate': 0.0019712017522703763, 'epoch': 1.04}\n",
      "{'loss': 1.4265, 'grad_norm': 2.8667097091674805, 'learning_rate': 0.0019692986056661357, 'epoch': 1.07}\n",
      "{'loss': 1.6041, 'grad_norm': 2.155552864074707, 'learning_rate': 0.00196733554839729, 'epoch': 1.09}\n",
      "{'loss': 1.5827, 'grad_norm': 2.2710423469543457, 'learning_rate': 0.001965312701797003, 'epoch': 1.12}\n",
      "{'loss': 1.5341, 'grad_norm': 2.4155514240264893, 'learning_rate': 0.0019632301908939125, 'epoch': 1.14}\n",
      "{'loss': 1.6141, 'grad_norm': 2.136378288269043, 'learning_rate': 0.0019610881444044027, 'epoch': 1.17}\n",
      "{'loss': 1.6191, 'grad_norm': 2.3268845081329346, 'learning_rate': 0.0019588866947246497, 'epoch': 1.19}\n",
      "{'loss': 1.6752, 'grad_norm': 2.53960919380188, 'learning_rate': 0.001956625977922438, 'epoch': 1.21}\n",
      "{'loss': 1.658, 'grad_norm': 2.844085931777954, 'learning_rate': 0.001954306133728749, 'epoch': 1.24}\n",
      "{'loss': 1.7032, 'grad_norm': 2.313408374786377, 'learning_rate': 0.0019519273055291265, 'epoch': 1.26}\n",
      "{'loss': 1.5535, 'grad_norm': 2.5252747535705566, 'learning_rate': 0.0019494896403548134, 'epoch': 1.29}\n",
      "{'loss': 1.7211, 'grad_norm': 3.9241652488708496, 'learning_rate': 0.0019469932888736632, 'epoch': 1.31}\n",
      "{'loss': 1.5706, 'grad_norm': 2.1327927112579346, 'learning_rate': 0.0019444384053808289, 'epoch': 1.33}\n",
      "{'loss': 1.5311, 'grad_norm': 4.043420314788818, 'learning_rate': 0.001941825147789225, 'epoch': 1.36}\n",
      "{'loss': 1.6314, 'grad_norm': 3.1109373569488525, 'learning_rate': 0.0019391536776197677, 'epoch': 1.38}\n",
      "{'loss': 1.5154, 'grad_norm': 2.757507801055908, 'learning_rate': 0.0019364241599913923, 'epoch': 1.41}\n",
      "{'loss': 1.5099, 'grad_norm': 2.6567230224609375, 'learning_rate': 0.001933636763610846, 'epoch': 1.43}\n",
      "{'loss': 1.5423, 'grad_norm': 2.8799259662628174, 'learning_rate': 0.001930791660762262, 'epoch': 1.46}\n",
      "{'loss': 1.4895, 'grad_norm': 2.6135616302490234, 'learning_rate': 0.0019278890272965096, 'epoch': 1.48}\n",
      "{'loss': 1.6972, 'grad_norm': 3.004692316055298, 'learning_rate': 0.0019249290426203252, 'epoch': 1.5}\n",
      "{'loss': 1.6213, 'grad_norm': 2.5921823978424072, 'learning_rate': 0.0019219118896852253, 'epoch': 1.53}\n",
      "{'loss': 1.5356, 'grad_norm': 2.747681140899658, 'learning_rate': 0.0019188377549761962, 'epoch': 1.55}\n",
      "{'loss': 1.5652, 'grad_norm': 2.4383060932159424, 'learning_rate': 0.0019157068285001692, 'epoch': 1.58}\n",
      "{'loss': 1.4623, 'grad_norm': 2.600524663925171, 'learning_rate': 0.001912519303774276, 'epoch': 1.6}\n",
      "{'loss': 1.6083, 'grad_norm': 3.0085978507995605, 'learning_rate': 0.0019092753778138885, 'epoch': 1.63}\n",
      "{'loss': 1.5804, 'grad_norm': 2.973440647125244, 'learning_rate': 0.0019059752511204398, 'epoch': 1.65}\n",
      "{'loss': 1.4292, 'grad_norm': 3.183617353439331, 'learning_rate': 0.0019026191276690341, 'epoch': 1.67}\n",
      "{'loss': 1.497, 'grad_norm': 3.1151883602142334, 'learning_rate': 0.0018992072148958368, 'epoch': 1.7}\n",
      "{'loss': 1.4901, 'grad_norm': 3.023715019226074, 'learning_rate': 0.0018957397236852556, 'epoch': 1.72}\n",
      "{'loss': 1.5886, 'grad_norm': 2.770195245742798, 'learning_rate': 0.001892216868356904, 'epoch': 1.75}\n",
      "{'loss': 1.5942, 'grad_norm': 2.696263074874878, 'learning_rate': 0.001888638866652356, 'epoch': 1.77}\n",
      "{'loss': 1.4621, 'grad_norm': 3.4174411296844482, 'learning_rate': 0.0018850059397216873, 'epoch': 1.8}\n",
      "{'loss': 1.5452, 'grad_norm': 2.3389668464660645, 'learning_rate': 0.0018813183121098073, 'epoch': 1.82}\n",
      "{'loss': 1.7978, 'grad_norm': 3.2738962173461914, 'learning_rate': 0.0018775762117425776, 'epoch': 1.84}\n",
      "{'loss': 1.4805, 'grad_norm': 3.153886079788208, 'learning_rate': 0.0018737798699127286, 'epoch': 1.87}\n",
      "{'loss': 1.3886, 'grad_norm': 3.2611324787139893, 'learning_rate': 0.0018699295212655596, 'epoch': 1.89}\n",
      "{'loss': 1.5912, 'grad_norm': 2.3083364963531494, 'learning_rate': 0.001866025403784439, 'epoch': 1.92}\n",
      "{'loss': 1.5025, 'grad_norm': 3.4555296897888184, 'learning_rate': 0.0018620677587760915, 'epoch': 1.94}\n",
      "{'loss': 1.6038, 'grad_norm': 3.42899751663208, 'learning_rate': 0.0018580568308556875, 'epoch': 1.97}\n",
      "{'loss': 1.5842, 'grad_norm': 2.856863498687744, 'learning_rate': 0.001853992867931721, 'epoch': 1.99}\n",
      "{'loss': 1.4034, 'grad_norm': 2.7935707569122314, 'learning_rate': 0.0018498761211906865, 'epoch': 2.01}\n",
      "{'loss': 1.4299, 'grad_norm': 3.3739311695098877, 'learning_rate': 0.001845706845081556, 'epoch': 2.04}\n",
      "{'loss': 1.1681, 'grad_norm': 3.1000466346740723, 'learning_rate': 0.0018414852973000502, 'epoch': 2.06}\n",
      "{'loss': 1.4737, 'grad_norm': 3.167056083679199, 'learning_rate': 0.0018372117387727108, 'epoch': 2.09}\n",
      "{'loss': 1.3745, 'grad_norm': 3.872535228729248, 'learning_rate': 0.0018328864336407734, 'epoch': 2.11}\n",
      "{'loss': 1.52, 'grad_norm': 3.590444564819336, 'learning_rate': 0.0018285096492438421, 'epoch': 2.14}\n",
      "{'loss': 1.2672, 'grad_norm': 3.123389959335327, 'learning_rate': 0.0018240816561033643, 'epoch': 2.16}\n",
      "{'loss': 1.538, 'grad_norm': 3.406888008117676, 'learning_rate': 0.0018196027279059116, 'epoch': 2.18}\n",
      "{'loss': 1.3757, 'grad_norm': 2.517613649368286, 'learning_rate': 0.0018150731414862623, 'epoch': 2.21}\n",
      "{'loss': 1.3321, 'grad_norm': 3.401793956756592, 'learning_rate': 0.001810493176810292, 'epoch': 2.23}\n",
      "{'loss': 1.4047, 'grad_norm': 3.268644332885742, 'learning_rate': 0.0018058631169576692, 'epoch': 2.26}\n",
      "{'loss': 1.393, 'grad_norm': 3.0881032943725586, 'learning_rate': 0.0018011832481043576, 'epoch': 2.28}\n",
      "{'loss': 1.4035, 'grad_norm': 3.321141481399536, 'learning_rate': 0.0017964538595049292, 'epoch': 2.31}\n",
      "{'loss': 1.3197, 'grad_norm': 3.46804141998291, 'learning_rate': 0.0017916752434746855, 'epoch': 2.33}\n",
      "{'loss': 1.4209, 'grad_norm': 3.1276965141296387, 'learning_rate': 0.00178684769537159, 'epoch': 2.35}\n",
      "{'loss': 1.4238, 'grad_norm': 4.014932155609131, 'learning_rate': 0.001781971513578013, 'epoch': 2.38}\n",
      "{'loss': 1.3907, 'grad_norm': 2.774768590927124, 'learning_rate': 0.0017770469994822884, 'epoch': 2.4}\n",
      "{'loss': 1.4319, 'grad_norm': 2.5022671222686768, 'learning_rate': 0.0017720744574600863, 'epoch': 2.43}\n",
      "{'loss': 1.3541, 'grad_norm': 3.616988182067871, 'learning_rate': 0.0017670541948555988, 'epoch': 2.45}\n",
      "{'loss': 1.3578, 'grad_norm': 3.31571364402771, 'learning_rate': 0.0017619865219625451, 'epoch': 2.48}\n",
      "{'loss': 1.5972, 'grad_norm': 3.090946674346924, 'learning_rate': 0.001756871752004992, 'epoch': 2.5}\n",
      "{'loss': 1.2651, 'grad_norm': 3.0376479625701904, 'learning_rate': 0.0017517102011179932, 'epoch': 2.52}\n",
      "{'loss': 1.56, 'grad_norm': 2.924619197845459, 'learning_rate': 0.001746502188328052, 'epoch': 2.55}\n",
      "{'loss': 1.4135, 'grad_norm': 26.80365753173828, 'learning_rate': 0.0017412480355334004, 'epoch': 2.57}\n",
      "{'loss': 1.563, 'grad_norm': 3.7812588214874268, 'learning_rate': 0.0017359480674841036, 'epoch': 2.6}\n",
      "{'loss': 1.4831, 'grad_norm': 2.68033504486084, 'learning_rate': 0.0017306026117619889, 'epoch': 2.62}\n",
      "{'loss': 1.4412, 'grad_norm': 3.713832378387451, 'learning_rate': 0.0017252119987603975, 'epoch': 2.65}\n",
      "{'loss': 1.5242, 'grad_norm': 3.0173561573028564, 'learning_rate': 0.0017197765616637636, 'epoch': 2.67}\n",
      "{'loss': 1.3359, 'grad_norm': 9.90650463104248, 'learning_rate': 0.001714296636427021, 'epoch': 2.69}\n",
      "{'loss': 1.3734, 'grad_norm': 4.082180023193359, 'learning_rate': 0.0017087725617548385, 'epoch': 2.72}\n",
      "{'loss': 1.4398, 'grad_norm': 3.5597424507141113, 'learning_rate': 0.001703204679080684, 'epoch': 2.74}\n",
      "{'loss': 1.4338, 'grad_norm': 3.737255811691284, 'learning_rate': 0.001697593332545723, 'epoch': 2.77}\n",
      "{'loss': 1.2488, 'grad_norm': 4.6983561515808105, 'learning_rate': 0.0016919388689775464, 'epoch': 2.79}\n",
      "{'loss': 1.2759, 'grad_norm': 3.5049960613250732, 'learning_rate': 0.0016862416378687337, 'epoch': 2.82}\n",
      "{'loss': 1.2694, 'grad_norm': 3.2915406227111816, 'learning_rate': 0.0016805019913552527, 'epoch': 2.84}\n",
      "{'loss': 1.4262, 'grad_norm': 3.4295151233673096, 'learning_rate': 0.0016747202841946927, 'epoch': 2.86}\n",
      "{'loss': 1.5651, 'grad_norm': 50.82432556152344, 'learning_rate': 0.0016688968737443397, 'epoch': 2.89}\n",
      "{'loss': 1.4547, 'grad_norm': 3.018275260925293, 'learning_rate': 0.0016630321199390867, 'epoch': 2.91}\n",
      "{'loss': 1.3894, 'grad_norm': 3.4991979598999023, 'learning_rate': 0.0016571263852691886, 'epoch': 2.94}\n",
      "{'loss': 1.4025, 'grad_norm': 4.299124240875244, 'learning_rate': 0.0016511800347578558, 'epoch': 2.96}\n",
      "{'loss': 1.4091, 'grad_norm': 3.2433483600616455, 'learning_rate': 0.0016451934359386934, 'epoch': 2.99}\n",
      "{'loss': 1.364, 'grad_norm': 3.2901952266693115, 'learning_rate': 0.0016391669588329849, 'epoch': 3.01}\n",
      "{'loss': 1.1864, 'grad_norm': 3.5135812759399414, 'learning_rate': 0.0016331009759268213, 'epoch': 3.03}\n",
      "{'loss': 1.1912, 'grad_norm': 2.8509292602539062, 'learning_rate': 0.0016269958621480786, 'epoch': 3.06}\n",
      "{'loss': 1.1916, 'grad_norm': 3.7274701595306396, 'learning_rate': 0.0016208519948432438, 'epoch': 3.08}\n",
      "{'loss': 1.1192, 'grad_norm': 3.714648962020874, 'learning_rate': 0.0016146697537540923, 'epoch': 3.11}\n",
      "{'loss': 1.319, 'grad_norm': 3.503204584121704, 'learning_rate': 0.0016084495209942175, 'epoch': 3.13}\n",
      "{'loss': 1.2456, 'grad_norm': 3.8145711421966553, 'learning_rate': 0.0016021916810254097, 'epoch': 3.16}\n",
      "{'loss': 1.1051, 'grad_norm': 3.6831111907958984, 'learning_rate': 0.0015958966206338979, 'epoch': 3.18}\n",
      "{'loss': 1.3293, 'grad_norm': 3.014288902282715, 'learning_rate': 0.0015895647289064395, 'epoch': 3.2}\n",
      "{'loss': 1.2138, 'grad_norm': 3.2327027320861816, 'learning_rate': 0.0015831963972062733, 'epoch': 3.23}\n",
      "{'loss': 1.2701, 'grad_norm': 3.3409125804901123, 'learning_rate': 0.0015767920191489299, 'epoch': 3.25}\n",
      "{'loss': 1.2761, 'grad_norm': 2.9641799926757812, 'learning_rate': 0.0015703519905779016, 'epoch': 3.28}\n",
      "{'loss': 1.1905, 'grad_norm': 3.4265317916870117, 'learning_rate': 0.0015638767095401778, 'epoch': 3.3}\n",
      "{'loss': 1.1891, 'grad_norm': 3.2440013885498047, 'learning_rate': 0.001557366576261642, 'epoch': 3.33}\n",
      "{'loss': 1.1758, 'grad_norm': 3.084728956222534, 'learning_rate': 0.001550821993122334, 'epoch': 3.35}\n",
      "{'loss': 1.2535, 'grad_norm': 4.088559150695801, 'learning_rate': 0.001544243364631579, 'epoch': 3.37}\n",
      "{'loss': 1.1963, 'grad_norm': 3.728253126144409, 'learning_rate': 0.0015376310974029872, 'epoch': 3.4}\n",
      "{'loss': 1.3355, 'grad_norm': 3.681954860687256, 'learning_rate': 0.0015309856001293203, 'epoch': 3.42}\n",
      "{'loss': 1.2679, 'grad_norm': 3.3373847007751465, 'learning_rate': 0.0015243072835572319, 'epoch': 3.45}\n",
      "{'loss': 1.1851, 'grad_norm': 4.031401634216309, 'learning_rate': 0.0015175965604618785, 'epoch': 3.47}\n",
      "{'loss': 1.3149, 'grad_norm': 4.648249626159668, 'learning_rate': 0.0015108538456214088, 'epoch': 3.5}\n",
      "{'loss': 1.3421, 'grad_norm': 3.3931639194488525, 'learning_rate': 0.0015040795557913246, 'epoch': 3.52}\n",
      "{'loss': 1.3109, 'grad_norm': 3.235105514526367, 'learning_rate': 0.001497274109678724, 'epoch': 3.54}\n",
      "{'loss': 1.1928, 'grad_norm': 3.906283140182495, 'learning_rate': 0.0014904379279164206, 'epoch': 3.57}\n",
      "{'loss': 1.1508, 'grad_norm': 2.966994285583496, 'learning_rate': 0.0014835714330369446, 'epoch': 3.59}\n",
      "{'loss': 1.1603, 'grad_norm': 2.9949378967285156, 'learning_rate': 0.0014766750494464274, 'epoch': 3.62}\n",
      "{'loss': 1.1039, 'grad_norm': 3.0885848999023438, 'learning_rate': 0.0014697492033983709, 'epoch': 3.64}\n",
      "{'loss': 1.2448, 'grad_norm': 2.7860634326934814, 'learning_rate': 0.001462794322967299, 'epoch': 3.67}\n",
      "{'loss': 1.2155, 'grad_norm': 2.4446847438812256, 'learning_rate': 0.0014558108380223012, 'epoch': 3.69}\n",
      "{'loss': 1.2984, 'grad_norm': 2.730422258377075, 'learning_rate': 0.0014487991802004624, 'epoch': 3.71}\n",
      "{'loss': 1.1419, 'grad_norm': 4.064876556396484, 'learning_rate': 0.0014417597828801831, 'epoch': 3.74}\n",
      "{'loss': 1.3316, 'grad_norm': 3.0579123497009277, 'learning_rate': 0.0014346930811543956, 'epoch': 3.76}\n",
      "{'loss': 1.1193, 'grad_norm': 2.909728765487671, 'learning_rate': 0.0014275995118036694, 'epoch': 3.79}\n",
      "{'loss': 1.1018, 'grad_norm': 2.213573694229126, 'learning_rate': 0.0014204795132692146, 'epoch': 3.81}\n",
      "{'loss': 1.2066, 'grad_norm': 3.8788769245147705, 'learning_rate': 0.001413333525625784, 'epoch': 3.83}\n",
      "{'loss': 1.2749, 'grad_norm': 3.9524927139282227, 'learning_rate': 0.0014061619905544724, 'epoch': 3.86}\n",
      "{'loss': 1.193, 'grad_norm': 3.0693726539611816, 'learning_rate': 0.0013989653513154163, 'epoch': 3.88}\n",
      "{'loss': 1.1719, 'grad_norm': 3.456993818283081, 'learning_rate': 0.0013917440527203976, 'epoch': 3.91}\n",
      "{'loss': 1.1707, 'grad_norm': 3.5740840435028076, 'learning_rate': 0.001384498541105349, 'epoch': 3.93}\n",
      "{'loss': 1.2267, 'grad_norm': 3.2740871906280518, 'learning_rate': 0.00137722926430277, 'epoch': 3.96}\n",
      "{'loss': 1.2021, 'grad_norm': 3.214113712310791, 'learning_rate': 0.0013699366716140434, 'epoch': 3.98}\n",
      "{'loss': 1.1446, 'grad_norm': 3.4642789363861084, 'learning_rate': 0.001362621213781667, 'epoch': 4.0}\n",
      "{'loss': 0.9217, 'grad_norm': 2.8221940994262695, 'learning_rate': 0.0013552833429613938, 'epoch': 4.03}\n",
      "{'loss': 1.0118, 'grad_norm': 3.029003381729126, 'learning_rate': 0.001347923512694284, 'epoch': 4.05}\n",
      "{'loss': 0.9926, 'grad_norm': 3.1865992546081543, 'learning_rate': 0.0013405421778786737, 'epoch': 4.08}\n",
      "{'loss': 0.9621, 'grad_norm': 3.5356435775756836, 'learning_rate': 0.0013331397947420576, 'epoch': 4.1}\n",
      "{'loss': 1.0485, 'grad_norm': 2.8076932430267334, 'learning_rate': 0.0013257168208128908, 'epoch': 4.13}\n",
      "{'loss': 1.1343, 'grad_norm': 4.584639072418213, 'learning_rate': 0.0013182737148923086, 'epoch': 4.15}\n",
      "{'loss': 0.9236, 'grad_norm': 2.7714884281158447, 'learning_rate': 0.0013108109370257714, 'epoch': 4.17}\n",
      "{'loss': 0.9769, 'grad_norm': 3.0081212520599365, 'learning_rate': 0.0013033289484746268, 'epoch': 4.2}\n",
      "{'loss': 0.9553, 'grad_norm': 3.3846476078033447, 'learning_rate': 0.0012958282116876025, 'epoch': 4.22}\n",
      "{'loss': 1.1032, 'grad_norm': 4.168910980224609, 'learning_rate': 0.0012883091902722219, 'epoch': 4.25}\n",
      "{'loss': 1.0214, 'grad_norm': 3.2527530193328857, 'learning_rate': 0.0012807723489661495, 'epoch': 4.27}\n",
      "{'loss': 0.9769, 'grad_norm': 2.454932928085327, 'learning_rate': 0.001273218153608466, 'epoch': 4.3}\n",
      "{'loss': 0.8977, 'grad_norm': 2.5460927486419678, 'learning_rate': 0.0012656470711108763, 'epoch': 4.32}\n",
      "{'loss': 0.9104, 'grad_norm': 2.8823201656341553, 'learning_rate': 0.0012580595694288504, 'epoch': 4.34}\n",
      "{'loss': 1.0291, 'grad_norm': 2.791180372238159, 'learning_rate': 0.0012504561175326985, 'epoch': 4.37}\n",
      "{'loss': 1.1, 'grad_norm': 3.65055251121521, 'learning_rate': 0.0012428371853785872, 'epoch': 4.39}\n",
      "{'loss': 0.866, 'grad_norm': 3.7112910747528076, 'learning_rate': 0.0012352032438794902, 'epoch': 4.42}\n",
      "{'loss': 0.8998, 'grad_norm': 2.6574625968933105, 'learning_rate': 0.0012275547648760825, 'epoch': 4.44}\n",
      "{'loss': 0.8832, 'grad_norm': 2.7404181957244873, 'learning_rate': 0.0012198922211075778, 'epoch': 4.47}\n",
      "{'loss': 1.0855, 'grad_norm': 3.896397829055786, 'learning_rate': 0.001212216086182508, 'epoch': 4.49}\n",
      "{'loss': 1.0682, 'grad_norm': 2.8627068996429443, 'learning_rate': 0.001204526834549451, 'epoch': 4.51}\n",
      "{'loss': 0.9795, 'grad_norm': 3.193505048751831, 'learning_rate': 0.0011968249414677054, 'epoch': 4.54}\n",
      "{'loss': 0.9046, 'grad_norm': 3.383388042449951, 'learning_rate': 0.0011891108829779164, 'epoch': 4.56}\n",
      "{'loss': 0.9038, 'grad_norm': 2.417799711227417, 'learning_rate': 0.0011813851358726513, 'epoch': 4.59}\n",
      "{'loss': 1.1087, 'grad_norm': 3.0786898136138916, 'learning_rate': 0.0011736481776669307, 'epoch': 4.61}\n",
      "{'loss': 0.9361, 'grad_norm': 2.881330966949463, 'learning_rate': 0.0011659004865687136, 'epoch': 4.64}\n",
      "{'loss': 0.8987, 'grad_norm': 3.2624893188476562, 'learning_rate': 0.001158142541449341, 'epoch': 4.66}\n",
      "{'loss': 0.9201, 'grad_norm': 3.019838333129883, 'learning_rate': 0.001150374821813937, 'epoch': 4.68}\n",
      "{'loss': 0.9535, 'grad_norm': 2.534193754196167, 'learning_rate': 0.0011425978077717709, 'epoch': 4.71}\n",
      "{'loss': 1.1675, 'grad_norm': 3.178591728210449, 'learning_rate': 0.0011348119800065842, 'epoch': 4.73}\n",
      "{'loss': 0.9449, 'grad_norm': 2.6964855194091797, 'learning_rate': 0.0011270178197468788, 'epoch': 4.76}\n",
      "{'loss': 0.818, 'grad_norm': 2.1227080821990967, 'learning_rate': 0.0011192158087361735, 'epoch': 4.78}\n",
      "{'loss': 0.9514, 'grad_norm': 2.6635630130767822, 'learning_rate': 0.0011114064292032282, 'epoch': 4.81}\n",
      "{'loss': 0.9043, 'grad_norm': 2.9345033168792725, 'learning_rate': 0.0011035901638322392, 'epoch': 4.83}\n",
      "{'loss': 0.9569, 'grad_norm': 2.7092556953430176, 'learning_rate': 0.0010957674957330042, 'epoch': 4.85}\n",
      "{'loss': 1.0747, 'grad_norm': 3.1653456687927246, 'learning_rate': 0.0010879389084110613, 'epoch': 4.88}\n",
      "{'loss': 0.9372, 'grad_norm': 2.313908100128174, 'learning_rate': 0.001080104885737807, 'epoch': 4.9}\n",
      "{'loss': 0.9447, 'grad_norm': 3.4362878799438477, 'learning_rate': 0.001072265911920587, 'epoch': 4.93}\n",
      "{'loss': 0.8852, 'grad_norm': 2.3274288177490234, 'learning_rate': 0.0010644224714727681, 'epoch': 4.95}\n",
      "{'loss': 1.0323, 'grad_norm': 2.422544240951538, 'learning_rate': 0.0010565750491837924, 'epoch': 4.98}\n",
      "{'loss': 1.1203, 'grad_norm': 2.673892021179199, 'learning_rate': 0.001048724130089212, 'epoch': 5.0}\n",
      "{'loss': 0.8487, 'grad_norm': 2.222938060760498, 'learning_rate': 0.0010408701994407117, 'epoch': 5.02}\n",
      "{'loss': 0.5996, 'grad_norm': 2.092986583709717, 'learning_rate': 0.0010330137426761135, 'epoch': 5.05}\n",
      "{'loss': 0.7029, 'grad_norm': 2.5790131092071533, 'learning_rate': 0.0010251552453893757, 'epoch': 5.07}\n",
      "{'loss': 0.7681, 'grad_norm': 3.035351276397705, 'learning_rate': 0.0010172951933005775, 'epoch': 5.1}\n",
      "{'loss': 0.8194, 'grad_norm': 2.7317395210266113, 'learning_rate': 0.0010094340722258967, 'epoch': 5.12}\n",
      "{'loss': 0.6507, 'grad_norm': 2.341858386993408, 'learning_rate': 0.0010015723680475846, 'epoch': 5.15}\n",
      "{'loss': 0.8468, 'grad_norm': 2.7222061157226562, 'learning_rate': 0.0009937105666839323, 'epoch': 5.17}\n",
      "{'loss': 0.8225, 'grad_norm': 2.597698450088501, 'learning_rate': 0.000985849154059238, 'epoch': 5.19}\n",
      "{'loss': 0.758, 'grad_norm': 2.1723389625549316, 'learning_rate': 0.0009779886160737727, 'epoch': 5.22}\n",
      "{'loss': 0.7541, 'grad_norm': 2.254692554473877, 'learning_rate': 0.000970129438573747, 'epoch': 5.24}\n",
      "{'loss': 0.7361, 'grad_norm': 1.805936336517334, 'learning_rate': 0.0009622721073212832, 'epoch': 5.27}\n",
      "{'loss': 0.7553, 'grad_norm': 2.5699760913848877, 'learning_rate': 0.000954417107964389, 'epoch': 5.29}\n",
      "{'loss': 0.8029, 'grad_norm': 2.3038229942321777, 'learning_rate': 0.0009465649260069428, 'epoch': 5.32}\n",
      "{'loss': 0.794, 'grad_norm': 2.1406900882720947, 'learning_rate': 0.0009387160467786839, 'epoch': 5.34}\n",
      "{'loss': 0.7973, 'grad_norm': 2.496623992919922, 'learning_rate': 0.0009308709554052158, 'epoch': 5.36}\n",
      "{'loss': 0.7688, 'grad_norm': 2.0818965435028076, 'learning_rate': 0.0009230301367780208, 'epoch': 5.39}\n",
      "{'loss': 0.8548, 'grad_norm': 2.20436429977417, 'learning_rate': 0.0009151940755244912, 'epoch': 5.41}\n",
      "{'loss': 0.7955, 'grad_norm': 1.9778891801834106, 'learning_rate': 0.000907363255977973, 'epoch': 5.44}\n",
      "{'loss': 0.7405, 'grad_norm': 2.4190244674682617, 'learning_rate': 0.000899538162147832, 'epoch': 5.46}\n",
      "{'loss': 0.8447, 'grad_norm': 2.080271005630493, 'learning_rate': 0.000891719277689538, 'epoch': 5.49}\n",
      "{'loss': 0.7509, 'grad_norm': 1.960098385810852, 'learning_rate': 0.0008839070858747696, 'epoch': 5.51}\n",
      "{'loss': 0.7618, 'grad_norm': 1.7828702926635742, 'learning_rate': 0.0008761020695615449, 'epoch': 5.53}\n",
      "{'loss': 0.7077, 'grad_norm': 2.117055892944336, 'learning_rate': 0.0008683047111643763, 'epoch': 5.56}\n",
      "{'loss': 0.7732, 'grad_norm': 2.3410420417785645, 'learning_rate': 0.0008605154926244543, 'epoch': 5.58}\n",
      "{'loss': 0.8366, 'grad_norm': 2.495999336242676, 'learning_rate': 0.0008527348953798589, 'epoch': 5.61}\n",
      "{'loss': 0.7174, 'grad_norm': 2.054037094116211, 'learning_rate': 0.0008449634003358021, 'epoch': 5.63}\n",
      "{'loss': 0.7914, 'grad_norm': 1.8303743600845337, 'learning_rate': 0.0008372014878349056, 'epoch': 5.66}\n",
      "{'loss': 0.5122, 'grad_norm': 1.4545420408248901, 'learning_rate': 0.0008294496376275104, 'epoch': 5.68}\n",
      "{'loss': 0.616, 'grad_norm': 1.6578985452651978, 'learning_rate': 0.000821708328842024, 'epoch': 5.7}\n",
      "{'loss': 0.6726, 'grad_norm': 2.0442183017730713, 'learning_rate': 0.000813978039955308, 'epoch': 5.73}\n",
      "{'loss': 0.6166, 'grad_norm': 1.7050756216049194, 'learning_rate': 0.0008062592487631022, 'epoch': 5.75}\n",
      "{'loss': 0.6696, 'grad_norm': 2.175164222717285, 'learning_rate': 0.0007985524323504948, 'epoch': 5.78}\n",
      "{'loss': 0.7339, 'grad_norm': 2.317681074142456, 'learning_rate': 0.0007908580670624326, 'epoch': 5.8}\n",
      "{'loss': 0.6194, 'grad_norm': 1.48867666721344, 'learning_rate': 0.0007831766284742808, 'epoch': 5.83}\n",
      "{'loss': 0.7491, 'grad_norm': 1.4979504346847534, 'learning_rate': 0.0007755085913624274, 'epoch': 5.85}\n",
      "{'loss': 0.751, 'grad_norm': 2.343489170074463, 'learning_rate': 0.0007678544296749383, 'epoch': 5.87}\n",
      "{'loss': 0.6468, 'grad_norm': 2.37526273727417, 'learning_rate': 0.0007602146165022639, 'epoch': 5.9}\n",
      "{'loss': 0.6525, 'grad_norm': 1.7392363548278809, 'learning_rate': 0.0007525896240479976, 'epoch': 5.92}\n",
      "{'loss': 0.7227, 'grad_norm': 1.7570469379425049, 'learning_rate': 0.0007449799235996895, 'epoch': 5.95}\n",
      "{'loss': 0.7375, 'grad_norm': 1.9682362079620361, 'learning_rate': 0.000737385985499718, 'epoch': 5.97}\n",
      "{'loss': 0.694, 'grad_norm': 2.2776682376861572, 'learning_rate': 0.000729808279116218, 'epoch': 6.0}\n",
      "{'loss': 0.5742, 'grad_norm': 2.8757340908050537, 'learning_rate': 0.0007222472728140695, 'epoch': 6.02}\n",
      "{'loss': 0.4817, 'grad_norm': 1.1869653463363647, 'learning_rate': 0.0007147034339259502, 'epoch': 6.04}\n",
      "{'loss': 0.4372, 'grad_norm': 1.308661937713623, 'learning_rate': 0.0007071772287234497, 'epoch': 6.07}\n",
      "{'loss': 0.5496, 'grad_norm': 1.5805526971817017, 'learning_rate': 0.0006996691223882496, 'epoch': 6.09}\n",
      "{'loss': 0.5188, 'grad_norm': 1.6758370399475098, 'learning_rate': 0.0006921795789833722, 'epoch': 6.12}\n",
      "{'loss': 0.545, 'grad_norm': 1.7688666582107544, 'learning_rate': 0.0006847090614244976, 'epoch': 6.14}\n",
      "{'loss': 0.4716, 'grad_norm': 1.5654659271240234, 'learning_rate': 0.0006772580314513508, 'epoch': 6.17}\n",
      "{'loss': 0.5286, 'grad_norm': 1.6001291275024414, 'learning_rate': 0.0006698269495991631, 'epoch': 6.19}\n",
      "{'loss': 0.4869, 'grad_norm': 1.7920746803283691, 'learning_rate': 0.0006624162751702077, 'epoch': 6.21}\n",
      "{'loss': 0.4929, 'grad_norm': 1.955145239830017, 'learning_rate': 0.0006550264662054095, 'epoch': 6.24}\n",
      "{'loss': 0.5706, 'grad_norm': 1.2226886749267578, 'learning_rate': 0.0006476579794560356, 'epoch': 6.26}\n",
      "{'loss': 0.4622, 'grad_norm': 2.6429929733276367, 'learning_rate': 0.0006403112703554642, 'epoch': 6.29}\n",
      "{'loss': 0.549, 'grad_norm': 2.223222017288208, 'learning_rate': 0.0006329867929910347, 'epoch': 6.31}\n",
      "{'loss': 0.5162, 'grad_norm': 2.063973903656006, 'learning_rate': 0.0006256850000759811, 'epoch': 6.33}\n",
      "{'loss': 0.5574, 'grad_norm': 1.9307444095611572, 'learning_rate': 0.0006184063429214515, 'epoch': 6.36}\n",
      "{'loss': 0.6862, 'grad_norm': 1.480406641960144, 'learning_rate': 0.0006111512714086126, 'epoch': 6.38}\n",
      "{'loss': 0.6056, 'grad_norm': 1.2864149808883667, 'learning_rate': 0.0006039202339608432, 'epoch': 6.41}\n",
      "{'loss': 0.4385, 'grad_norm': 1.6195734739303589, 'learning_rate': 0.0005967136775160187, 'epoch': 6.43}\n",
      "{'loss': 0.5546, 'grad_norm': 1.3781262636184692, 'learning_rate': 0.0005895320474988864, 'epoch': 6.46}\n",
      "{'loss': 0.5831, 'grad_norm': 1.994454264640808, 'learning_rate': 0.0005823757877935332, 'epoch': 6.48}\n",
      "{'loss': 0.5748, 'grad_norm': 1.8459476232528687, 'learning_rate': 0.0005752453407159521, 'epoch': 6.5}\n",
      "{'loss': 0.4873, 'grad_norm': 1.2534654140472412, 'learning_rate': 0.0005681411469867026, 'epoch': 6.53}\n",
      "{'loss': 0.5402, 'grad_norm': 1.8255020380020142, 'learning_rate': 0.0005610636457036693, 'epoch': 6.55}\n",
      "{'loss': 0.4719, 'grad_norm': 1.7292368412017822, 'learning_rate': 0.0005540132743149242, 'epoch': 6.58}\n",
      "{'loss': 0.5443, 'grad_norm': 1.7298967838287354, 'learning_rate': 0.0005469904685916861, 'epoch': 6.6}\n",
      "{'loss': 0.6286, 'grad_norm': 1.3070487976074219, 'learning_rate': 0.0005399956626013892, 'epoch': 6.63}\n",
      "{'loss': 0.4263, 'grad_norm': 2.2857248783111572, 'learning_rate': 0.000533029288680852, 'epoch': 6.65}\n",
      "{'loss': 0.5558, 'grad_norm': 1.837956190109253, 'learning_rate': 0.0005260917774095568, 'epoch': 6.67}\n",
      "{'loss': 0.4957, 'grad_norm': 1.5462270975112915, 'learning_rate': 0.0005191835575830352, 'epoch': 6.7}\n",
      "{'loss': 0.4309, 'grad_norm': 1.228866457939148, 'learning_rate': 0.0005123050561863657, 'epoch': 6.72}\n",
      "{'loss': 0.5394, 'grad_norm': 1.6632441282272339, 'learning_rate': 0.0005054566983677814, 'epoch': 6.75}\n",
      "{'loss': 0.4944, 'grad_norm': 0.9916312098503113, 'learning_rate': 0.0004986389074123943, 'epoch': 6.77}\n",
      "{'loss': 0.4621, 'grad_norm': 2.0038881301879883, 'learning_rate': 0.0004918521047160309, 'epoch': 6.8}\n",
      "{'loss': 0.5177, 'grad_norm': 1.3081400394439697, 'learning_rate': 0.0004850967097591873, 'epoch': 6.82}\n",
      "{'loss': 0.5697, 'grad_norm': 1.5464590787887573, 'learning_rate': 0.00047837314008110224, 'epoch': 6.84}\n",
      "{'loss': 0.6266, 'grad_norm': 1.402265191078186, 'learning_rate': 0.0004716818112539485, 'epoch': 6.87}\n",
      "{'loss': 0.4992, 'grad_norm': 1.4866955280303955, 'learning_rate': 0.00046502313685714856, 'epoch': 6.89}\n",
      "{'loss': 0.5251, 'grad_norm': 1.182921051979065, 'learning_rate': 0.00045839752845181, 'epoch': 6.92}\n",
      "{'loss': 0.4916, 'grad_norm': 1.0860097408294678, 'learning_rate': 0.00045180539555529035, 'epoch': 6.94}\n",
      "{'loss': 0.4682, 'grad_norm': 1.630047082901001, 'learning_rate': 0.00044524714561588286, 'epoch': 6.97}\n",
      "{'loss': 0.528, 'grad_norm': 1.3748027086257935, 'learning_rate': 0.0004387231839876349, 'epoch': 6.99}\n",
      "{'loss': 0.442, 'grad_norm': 1.1497358083724976, 'learning_rate': 0.0004322339139052921, 'epoch': 7.01}\n",
      "{'loss': 0.3712, 'grad_norm': 1.2687321901321411, 'learning_rate': 0.0004257797364593767, 'epoch': 7.04}\n",
      "{'loss': 0.3878, 'grad_norm': 0.8617330193519592, 'learning_rate': 0.0004193610505713946, 'epoch': 7.06}\n",
      "{'loss': 0.3424, 'grad_norm': 1.5814120769500732, 'learning_rate': 0.00041297825296918145, 'epoch': 7.09}\n",
      "{'loss': 0.3411, 'grad_norm': 1.2763888835906982, 'learning_rate': 0.00040663173816237976, 'epoch': 7.11}\n",
      "{'loss': 0.3385, 'grad_norm': 0.8334551453590393, 'learning_rate': 0.0004003218984180552, 'epoch': 7.14}\n",
      "{'loss': 0.3952, 'grad_norm': 1.7100392580032349, 'learning_rate': 0.00039404912373645185, 'epoch': 7.16}\n",
      "{'loss': 0.349, 'grad_norm': 1.1569594144821167, 'learning_rate': 0.00038781380182688666, 'epoch': 7.18}\n",
      "{'loss': 0.3305, 'grad_norm': 1.3089405298233032, 'learning_rate': 0.00038161631808378483, 'epoch': 7.21}\n",
      "{'loss': 0.3811, 'grad_norm': 1.5102801322937012, 'learning_rate': 0.00037545705556286127, 'epoch': 7.23}\n",
      "{'loss': 0.4379, 'grad_norm': 1.706642508506775, 'learning_rate': 0.0003693363949574429, 'epoch': 7.26}\n",
      "{'loss': 0.3273, 'grad_norm': 1.1933305263519287, 'learning_rate': 0.0003632547145749395, 'epoch': 7.28}\n",
      "{'loss': 0.3377, 'grad_norm': 1.4423991441726685, 'learning_rate': 0.0003572123903134606, 'epoch': 7.31}\n",
      "{'loss': 0.3445, 'grad_norm': 0.8384367823600769, 'learning_rate': 0.0003512097956385827, 'epoch': 7.33}\n",
      "{'loss': 0.3628, 'grad_norm': 1.7496247291564941, 'learning_rate': 0.00034524730156026517, 'epoch': 7.35}\n",
      "{'loss': 0.3508, 'grad_norm': 1.3684040307998657, 'learning_rate': 0.00033932527660991875, 'epoch': 7.38}\n",
      "{'loss': 0.3407, 'grad_norm': 1.1281988620758057, 'learning_rate': 0.0003334440868176282, 'epoch': 7.4}\n",
      "{'loss': 0.3209, 'grad_norm': 1.4681892395019531, 'learning_rate': 0.00032760409568952765, 'epoch': 7.43}\n",
      "{'loss': 0.3406, 'grad_norm': 1.284050703048706, 'learning_rate': 0.0003218056641853336, 'epoch': 7.45}\n",
      "{'loss': 0.371, 'grad_norm': 1.1349763870239258, 'learning_rate': 0.00031604915069603437, 'epoch': 7.48}\n",
      "{'loss': 0.4278, 'grad_norm': 1.1777853965759277, 'learning_rate': 0.0003103349110217384, 'epoch': 7.5}\n",
      "{'loss': 0.3509, 'grad_norm': 1.412739872932434, 'learning_rate': 0.0003046632983496823, 'epoch': 7.52}\n",
      "{'loss': 0.34, 'grad_norm': 1.600417971611023, 'learning_rate': 0.0002990346632324027, 'epoch': 7.55}\n",
      "{'loss': 0.2941, 'grad_norm': 1.384491205215454, 'learning_rate': 0.00029344935356606774, 'epoch': 7.57}\n",
      "{'loss': 0.3419, 'grad_norm': 1.2351077795028687, 'learning_rate': 0.0002879077145689746, 'epoch': 7.6}\n",
      "{'loss': 0.4179, 'grad_norm': 1.2222100496292114, 'learning_rate': 0.0002824100887602121, 'epoch': 7.62}\n",
      "{'loss': 0.3684, 'grad_norm': 1.3001196384429932, 'learning_rate': 0.000276956815938491, 'epoch': 7.65}\n",
      "{'loss': 0.3549, 'grad_norm': 1.5026956796646118, 'learning_rate': 0.0002715482331611393, 'epoch': 7.67}\n",
      "{'loss': 0.3923, 'grad_norm': 1.0683341026306152, 'learning_rate': 0.00026618467472327246, 'epoch': 7.69}\n",
      "{'loss': 0.3121, 'grad_norm': 1.225035309791565, 'learning_rate': 0.000260866472137129, 'epoch': 7.72}\n",
      "{'loss': 0.4139, 'grad_norm': 1.1293994188308716, 'learning_rate': 0.00025559395411158117, 'epoch': 7.74}\n",
      "{'loss': 0.3993, 'grad_norm': 1.6008602380752563, 'learning_rate': 0.0002503674465318175, 'epoch': 7.77}\n",
      "{'loss': 0.2363, 'grad_norm': 0.7787207961082458, 'learning_rate': 0.00024518727243920115, 'epoch': 7.79}\n",
      "{'loss': 0.3479, 'grad_norm': 1.4501817226409912, 'learning_rate': 0.00024005375201130276, 'epoch': 7.82}\n",
      "{'loss': 0.3116, 'grad_norm': 1.6493703126907349, 'learning_rate': 0.00023496720254211024, 'epoch': 7.84}\n",
      "{'loss': 0.3971, 'grad_norm': 1.211187481880188, 'learning_rate': 0.00022992793842241898, 'epoch': 7.86}\n",
      "{'loss': 0.3127, 'grad_norm': 1.3990319967269897, 'learning_rate': 0.0002249362711203985, 'epoch': 7.89}\n",
      "{'loss': 0.3041, 'grad_norm': 0.7919015884399414, 'learning_rate': 0.0002199925091623418, 'epoch': 7.91}\n",
      "{'loss': 0.3308, 'grad_norm': 1.3235814571380615, 'learning_rate': 0.00021509695811359553, 'epoch': 7.94}\n",
      "{'loss': 0.2954, 'grad_norm': 0.940923273563385, 'learning_rate': 0.00021024992055967428, 'epoch': 7.96}\n",
      "{'loss': 0.3969, 'grad_norm': 0.932385265827179, 'learning_rate': 0.00020545169608755654, 'epoch': 7.99}\n",
      "{'loss': 0.2949, 'grad_norm': 0.8715022802352905, 'learning_rate': 0.00020070258126717, 'epoch': 8.01}\n",
      "{'loss': 0.2301, 'grad_norm': 1.1491482257843018, 'learning_rate': 0.00019600286963305957, 'epoch': 8.03}\n",
      "{'loss': 0.2592, 'grad_norm': 1.5508954524993896, 'learning_rate': 0.00019135285166624517, 'epoch': 8.06}\n",
      "{'loss': 0.2535, 'grad_norm': 1.4722241163253784, 'learning_rate': 0.00018675281477626703, 'epoch': 8.08}\n",
      "{'loss': 0.2293, 'grad_norm': 1.084089994430542, 'learning_rate': 0.00018220304328342253, 'epoch': 8.11}\n",
      "{'loss': 0.2427, 'grad_norm': 0.8531709909439087, 'learning_rate': 0.00017770381840119076, 'epoch': 8.13}\n",
      "{'loss': 0.2488, 'grad_norm': 0.7279072999954224, 'learning_rate': 0.00017325541821885382, 'epoch': 8.16}\n",
      "{'loss': 0.223, 'grad_norm': 0.6018041968345642, 'learning_rate': 0.00016885811768430658, 'epoch': 8.18}\n",
      "{'loss': 0.2331, 'grad_norm': 1.453589677810669, 'learning_rate': 0.00016451218858706373, 'epoch': 8.2}\n",
      "{'loss': 0.2415, 'grad_norm': 0.8106063604354858, 'learning_rate': 0.00016021789954146038, 'epoch': 8.23}\n",
      "{'loss': 0.2482, 'grad_norm': 0.9645687341690063, 'learning_rate': 0.00015597551597004966, 'epoch': 8.25}\n",
      "{'loss': 0.2195, 'grad_norm': 1.0091990232467651, 'learning_rate': 0.00015178530008719794, 'epoch': 8.28}\n",
      "{'loss': 0.234, 'grad_norm': 1.2328004837036133, 'learning_rate': 0.0001476475108828762, 'epoch': 8.3}\n",
      "{'loss': 0.2127, 'grad_norm': 0.6487879157066345, 'learning_rate': 0.00014356240410665433, 'epoch': 8.33}\n",
      "{'loss': 0.2111, 'grad_norm': 1.4848376512527466, 'learning_rate': 0.00013953023225189243, 'epoch': 8.35}\n",
      "{'loss': 0.258, 'grad_norm': 0.6438143253326416, 'learning_rate': 0.00013555124454013513, 'epoch': 8.37}\n",
      "{'loss': 0.2263, 'grad_norm': 1.1622482538223267, 'learning_rate': 0.00013162568690570743, 'epoch': 8.4}\n",
      "{'loss': 0.2025, 'grad_norm': 0.7902534008026123, 'learning_rate': 0.0001277538019805139, 'epoch': 8.42}\n",
      "{'loss': 0.2409, 'grad_norm': 0.70704585313797, 'learning_rate': 0.000123935829079042, 'epoch': 8.45}\n",
      "{'loss': 0.195, 'grad_norm': 0.907349169254303, 'learning_rate': 0.00012017200418357077, 'epoch': 8.47}\n",
      "{'loss': 0.2683, 'grad_norm': 0.6772291660308838, 'learning_rate': 0.00011646255992958466, 'epoch': 8.5}\n",
      "{'loss': 0.1862, 'grad_norm': 0.8549515008926392, 'learning_rate': 0.00011280772559139585, 'epoch': 8.52}\n",
      "{'loss': 0.2318, 'grad_norm': 0.9840967655181885, 'learning_rate': 0.00010920772706797167, 'epoch': 8.54}\n",
      "{'loss': 0.2587, 'grad_norm': 1.5188755989074707, 'learning_rate': 0.00010566278686897279, 'epoch': 8.57}\n",
      "{'loss': 0.2064, 'grad_norm': 0.7622356414794922, 'learning_rate': 0.00010217312410100088, 'epoch': 8.59}\n",
      "{'loss': 0.241, 'grad_norm': 1.2407053709030151, 'learning_rate': 9.873895445405524e-05, 'epoch': 8.62}\n",
      "{'loss': 0.2192, 'grad_norm': 1.3071188926696777, 'learning_rate': 9.536049018820191e-05, 'epoch': 8.64}\n",
      "{'loss': 0.239, 'grad_norm': 0.9619941711425781, 'learning_rate': 9.203794012045375e-05, 'epoch': 8.67}\n",
      "{'loss': 0.2347, 'grad_norm': 1.3367770910263062, 'learning_rate': 8.87715096118642e-05, 'epoch': 8.69}\n",
      "{'loss': 0.2187, 'grad_norm': 0.8846741914749146, 'learning_rate': 8.55614005548343e-05, 'epoch': 8.71}\n",
      "{'loss': 0.2466, 'grad_norm': 0.6545613408088684, 'learning_rate': 8.240781136063346e-05, 'epoch': 8.74}\n",
      "{'loss': 0.2063, 'grad_norm': 1.0468075275421143, 'learning_rate': 7.931093694713687e-05, 'epoch': 8.76}\n",
      "{'loss': 0.2084, 'grad_norm': 1.1066341400146484, 'learning_rate': 7.627096872677741e-05, 'epoch': 8.79}\n",
      "{'loss': 0.228, 'grad_norm': 1.2080689668655396, 'learning_rate': 7.328809459471498e-05, 'epoch': 8.81}\n",
      "{'loss': 0.2094, 'grad_norm': 0.8161709308624268, 'learning_rate': 7.036249891722279e-05, 'epoch': 8.83}\n",
      "{'loss': 0.2489, 'grad_norm': 1.0951616764068604, 'learning_rate': 6.749436252029262e-05, 'epoch': 8.86}\n",
      "{'loss': 0.1884, 'grad_norm': 0.5211155414581299, 'learning_rate': 6.468386267845716e-05, 'epoch': 8.88}\n",
      "{'loss': 0.2492, 'grad_norm': 1.1826876401901245, 'learning_rate': 6.193117310383411e-05, 'epoch': 8.91}\n",
      "{'loss': 0.2075, 'grad_norm': 0.41586631536483765, 'learning_rate': 5.923646393538906e-05, 'epoch': 8.93}\n",
      "{'loss': 0.2422, 'grad_norm': 0.6098623275756836, 'learning_rate': 5.659990172841867e-05, 'epoch': 8.96}\n",
      "{'loss': 0.2492, 'grad_norm': 0.7846497297286987, 'learning_rate': 5.402164944425758e-05, 'epoch': 8.98}\n",
      "{'loss': 0.24, 'grad_norm': 0.7666987180709839, 'learning_rate': 5.150186644020483e-05, 'epoch': 9.0}\n",
      "{'loss': 0.1555, 'grad_norm': 0.6512717604637146, 'learning_rate': 4.904070845967468e-05, 'epoch': 9.03}\n",
      "{'loss': 0.1601, 'grad_norm': 0.830692708492279, 'learning_rate': 4.66383276225707e-05, 'epoch': 9.05}\n",
      "{'loss': 0.1767, 'grad_norm': 0.9113087058067322, 'learning_rate': 4.429487241588304e-05, 'epoch': 9.08}\n",
      "{'loss': 0.1688, 'grad_norm': 0.8750074505805969, 'learning_rate': 4.20104876845111e-05, 'epoch': 9.1}\n",
      "{'loss': 0.1751, 'grad_norm': 0.857062041759491, 'learning_rate': 3.97853146223105e-05, 'epoch': 9.13}\n",
      "{'loss': 0.1737, 'grad_norm': 0.9044235944747925, 'learning_rate': 3.761949076336646e-05, 'epoch': 9.15}\n",
      "{'loss': 0.1543, 'grad_norm': 0.8817272186279297, 'learning_rate': 3.551314997349297e-05, 'epoch': 9.17}\n",
      "{'loss': 0.1597, 'grad_norm': 0.7240113615989685, 'learning_rate': 3.346642244195863e-05, 'epoch': 9.2}\n",
      "{'loss': 0.177, 'grad_norm': 0.6942936182022095, 'learning_rate': 3.147943467344017e-05, 'epoch': 9.22}\n",
      "{'loss': 0.1985, 'grad_norm': 0.6833184957504272, 'learning_rate': 2.9552309480203042e-05, 'epoch': 9.25}\n",
      "{'loss': 0.1638, 'grad_norm': 0.7326728701591492, 'learning_rate': 2.7685165974510984e-05, 'epoch': 9.27}\n",
      "{'loss': 0.1743, 'grad_norm': 0.36568760871887207, 'learning_rate': 2.5878119561263737e-05, 'epoch': 9.3}\n",
      "{'loss': 0.1922, 'grad_norm': 0.8005706667900085, 'learning_rate': 2.4131281930864003e-05, 'epoch': 9.32}\n",
      "{'loss': 0.1849, 'grad_norm': 0.6699831485748291, 'learning_rate': 2.2444761052313855e-05, 'epoch': 9.34}\n",
      "{'loss': 0.181, 'grad_norm': 0.9109475612640381, 'learning_rate': 2.0818661166542074e-05, 'epoch': 9.37}\n",
      "{'loss': 0.1693, 'grad_norm': 0.9896458983421326, 'learning_rate': 1.9253082779960407e-05, 'epoch': 9.39}\n",
      "{'loss': 0.1848, 'grad_norm': 1.0476274490356445, 'learning_rate': 1.7748122658251876e-05, 'epoch': 9.42}\n",
      "{'loss': 0.1826, 'grad_norm': 0.9365158677101135, 'learning_rate': 1.6303873820389893e-05, 'epoch': 9.44}\n",
      "{'loss': 0.1756, 'grad_norm': 1.1103812456130981, 'learning_rate': 1.4920425532888526e-05, 'epoch': 9.47}\n",
      "{'loss': 0.2147, 'grad_norm': 1.438639521598816, 'learning_rate': 1.3597863304285474e-05, 'epoch': 9.49}\n",
      "{'loss': 0.1924, 'grad_norm': 0.993679940700531, 'learning_rate': 1.2336268879856727e-05, 'epoch': 9.51}\n",
      "{'loss': 0.1701, 'grad_norm': 0.545809268951416, 'learning_rate': 1.1135720236564283e-05, 'epoch': 9.54}\n",
      "{'loss': 0.167, 'grad_norm': 0.6189736127853394, 'learning_rate': 9.996291578236227e-06, 'epoch': 9.56}\n",
      "{'loss': 0.1831, 'grad_norm': 0.6565595269203186, 'learning_rate': 8.91805333098039e-06, 'epoch': 9.59}\n",
      "{'loss': 0.1847, 'grad_norm': 0.49294766783714294, 'learning_rate': 7.90107213883151e-06, 'epoch': 9.61}\n",
      "{'loss': 0.1671, 'grad_norm': 0.49192073941230774, 'learning_rate': 6.945410859632295e-06, 'epoch': 9.64}\n",
      "{'loss': 0.1764, 'grad_norm': 0.6514846086502075, 'learning_rate': 6.0511285611477566e-06, 'epoch': 9.66}\n",
      "{'loss': 0.1579, 'grad_norm': 0.8644094467163086, 'learning_rate': 5.218280517414686e-06, 'epoch': 9.68}\n",
      "{'loss': 0.1716, 'grad_norm': 0.7525122761726379, 'learning_rate': 4.44691820532539e-06, 'epoch': 9.71}\n",
      "{'loss': 0.1759, 'grad_norm': 0.3987078070640564, 'learning_rate': 3.737089301445562e-06, 'epoch': 9.73}\n",
      "{'loss': 0.1561, 'grad_norm': 0.726285457611084, 'learning_rate': 3.0888376790679794e-06, 'epoch': 9.76}\n",
      "{'loss': 0.1645, 'grad_norm': 0.7913714647293091, 'learning_rate': 2.5022034055003363e-06, 'epoch': 9.78}\n",
      "{'loss': 0.1606, 'grad_norm': 0.6447380781173706, 'learning_rate': 1.977222739588891e-06, 'epoch': 9.81}\n",
      "{'loss': 0.164, 'grad_norm': 0.8163578510284424, 'learning_rate': 1.513928129477593e-06, 'epoch': 9.83}\n",
      "{'loss': 0.1769, 'grad_norm': 0.9720155000686646, 'learning_rate': 1.1123482106021321e-06, 'epoch': 9.85}\n",
      "{'loss': 0.1526, 'grad_norm': 1.0369057655334473, 'learning_rate': 7.725078039205746e-07, 'epoch': 9.88}\n",
      "{'loss': 0.1752, 'grad_norm': 0.8371939659118652, 'learning_rate': 4.944279143784813e-07, 'epoch': 9.9}\n",
      "{'loss': 0.1568, 'grad_norm': 0.8248549103736877, 'learning_rate': 2.7812572961127825e-07, 'epoch': 9.93}\n",
      "{'loss': 0.1486, 'grad_norm': 0.49455350637435913, 'learning_rate': 1.2361461888166225e-07, 'epoch': 9.95}\n",
      "{'loss': 0.1931, 'grad_norm': 0.5754775404930115, 'learning_rate': 3.090413225315114e-08, 'epoch': 9.98}\n",
      "{'loss': 0.1749, 'grad_norm': 1.1069355010986328, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 5146.7989, 'train_samples_per_second': 6.404, 'train_steps_per_second': 0.8, 'train_loss': 0.9037208723501094, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "num_train_epochs = 10\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,      \n",
    "    per_device_train_batch_size=4,          \n",
    "    gradient_accumulation_steps=2,          \n",
    "    optim=\"paged_adamw_32bit\",              \n",
    "    save_steps=0,                           \n",
    "    logging_steps=10,                       \n",
    "    learning_rate=2e-3,                     \n",
    "    weight_decay=0.001,                     \n",
    "    fp16=False,                            \n",
    "    bf16=False,                             \n",
    "    max_grad_norm=0.3,                     \n",
    "    max_steps=-1,                           \n",
    "    warmup_ratio=0.03,                      \n",
    "    group_by_length=True,                   \n",
    "    lr_scheduler_type=\"cosine\",           \n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=new_df,\n",
    "    peft_config=peft_config,                \n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=100,                    \n",
    "    tokenizer=tokenizer,                   \n",
    "    args=training_arguments,                \n",
    "    packing=False,                          \n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca0ad5-8d52-49d5-af87-3d6ed0287c8c",
   "metadata": {},
   "source": [
    "# Testing the fine tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a412f0a4-510c-4fb3-b387-ee9c2057d4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] I have a multi-line string literal that I want to do an operation on each line, like so: inputString = '''Line 1 Line 2 Line 3''' I want to do something like the following: for line in inputString: doStuff() [/INST] (You could also use the builtin function map:\n",
      "def map(a):\n",
      "    return map(lambda x: x[0]+a]\n",
      "\n",
      ") \n",
      "\n",
      ")  - only works for Python 2.7 or later\n",
      ") \n",
      "\n",
      "as you can't get the whole line right away without the'map'.\n",
      "for line in inputLine:\n",
      "    doSomething(line)\n",
      "\n",
      ") \n",
      "\n",
      "# Example 1\n",
      "line_1 line_2 line_3\n",
      "do-something(line_1) \n",
      "\n",
      "can also be used for dynamic operations on multiple lines (like assigning multiple variables values to a collection\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"I have a multi-line string literal that I want to do an operation on each line, like so: inputString = '''Line 1 Line 2 Line 3''' I want to do something like the following: for line in inputString: doStuff()\"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7129f-ae94-40e5-b596-cbcdf41aef5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
