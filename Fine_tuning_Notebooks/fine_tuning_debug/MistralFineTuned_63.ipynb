{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1224787d-0c6e-4d0b-8c43-a290605320e7",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e238df-4a49-4160-877a-9fc6f5ba5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/zealous_easley/.local/lib/python3.8/site-packages (0.1.99)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# we use the latest version of transformers, peft, and accelerate\n",
    "!pip install -q accelerate peft transformers\n",
    "\n",
    "# install bitsandbytes for quantization\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "# install trl for the SFT library\n",
    "!pip install -q trl\n",
    "\n",
    "# we need sentencepiece for the llama2 slow tokenizer\n",
    "!pip install sentencepiece\n",
    "\n",
    "# we need einops, used by falcon-7b, llama-2 etc\n",
    "# einops (einsteinops) is used to simplify tensorops by making them readable\n",
    "!pip install -q -U einops\n",
    "\n",
    "# we need to install datasets for our training dataset\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a44e3-f953-4991-9803-17e36888abe6",
   "metadata": {},
   "source": [
    "# Downloading Mistral 7B Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f90b76c-cc3b-425a-97b1-b5d3b63db6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that we want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# The instruction dataset to use found on HuggingFace\n",
    "dataset_name = \"KonradSzafer/stackoverflow_python_preprocessed\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Mistral-7B-Stackoverflow\"\n",
    "output_dir = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6573504-3d44-49b1-8a1c-0f8349231dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 09:15:18.824152: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-06 09:15:18.874059: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dd4a634c9040c1943a1de90e33c8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (llama2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19402d77-f9e8-4ec7-a4b2-7e29e2be2fc9",
   "metadata": {},
   "source": [
    "# Testing it on some prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be93cca-f7cd-42ec-9a28-b72f2531b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] I have this Python application that gets stuck from time to time and I can't find out where. Is there any way to signal Python interpreter to show you the exact code that's running? Some kind of on-the-fly stacktrace? Related questions: Print current call stack from a method in Python code Check what a running process is doing: print stack trace of an uninstrumented Python program [/INST] In Python, there isn't a built-in mechanism to print a stack trace at a specific point in your code like there is in some other languages (such as Java or C++). However, you can use various methods to help identify where your code is getting stuck.\n",
      "\n",
      "1. **Manually add stack traces:** The simplest way to get a stack trace when your code is getting stuck is to add a stack trace manually at the point where you suspect the issue is occurring. You can use the `traceback` module\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "\n",
    "prompt = \"I have this Python application that gets stuck from time to time and I can't find out where. Is there any way to signal Python interpreter to show you the exact code that's running? Some kind of on-the-fly stacktrace? Related questions: Print current call stack from a method in Python code Check what a running process is doing: print stack trace of an uninstrumented Python program\"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a9cd2-6d11-4b5f-aeb5-f7d1598a6456",
   "metadata": {},
   "source": [
    "# Fine tuning the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f882dce-5f31-458a-b04c-01460b88dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train[:200]\")\n",
    "\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Create the new 'text' column by concatenating the formatted text\n",
    "df['text'] = '<s>[INST] ' + df['question'] + ' [/INST] ' + df['answer'] + ' </s>'\n",
    "\n",
    "# Keep only the 'text' column in the new dataset\n",
    "new_df = df[['text']]\n",
    "# Convert DataFrame to dataset\n",
    "new_df = Dataset.from_pandas(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3198930-d39c-4e7b-812a-1a556b4badca",
   "metadata": {},
   "source": [
    "We used the whole dataset for training since it is very small. Also fine tuning a LLM on a programming language is a unique training technique and it does require the model to be trained on every code it can be trained on,  unlike other training techniques where we can split our dataset into training and validating sets. Finally to validate the model & measure its performance we use Unit testing of the programming language, unfotunately we don't have that for OPL PSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b23454-2f3d-405c-85c1-ac4436fe9c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laughing_bhabha/.local/lib/python3.8/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9ef4061e4a4babb83a9ff4a1602e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5166, 'grad_norm': 1.2386070489883423, 'learning_rate': 0.0006666666666666666, 'epoch': 0.1}\n",
      "{'loss': 2.3965, 'grad_norm': 1.7245467901229858, 'learning_rate': 0.0013333333333333333, 'epoch': 0.2}\n",
      "{'loss': 1.8916, 'grad_norm': 2.782966136932373, 'learning_rate': 0.002, 'epoch': 0.3}\n",
      "{'loss': 2.0906, 'grad_norm': 1.9421788454055786, 'learning_rate': 0.0019994755690455153, 'epoch': 0.4}\n",
      "{'loss': 2.214, 'grad_norm': 5.066443920135498, 'learning_rate': 0.0019979028262377117, 'epoch': 0.5}\n",
      "{'loss': 2.1276, 'grad_norm': 8.79931354522705, 'learning_rate': 0.0019952834211666138, 'epoch': 0.6}\n",
      "{'loss': 2.3057, 'grad_norm': 3.6244630813598633, 'learning_rate': 0.001991620101226425, 'epoch': 0.7}\n",
      "{'loss': 1.918, 'grad_norm': 5.809851169586182, 'learning_rate': 0.0019869167087338906, 'epoch': 0.8}\n",
      "{'loss': 1.556, 'grad_norm': 2.537376642227173, 'learning_rate': 0.0019811781768982392, 'epoch': 0.9}\n",
      "{'loss': 1.7934, 'grad_norm': 4.265195369720459, 'learning_rate': 0.001974410524646926, 'epoch': 1.0}\n",
      "{'loss': 1.495, 'grad_norm': 2.1352081298828125, 'learning_rate': 0.001966620850312611, 'epoch': 1.1}\n",
      "{'loss': 1.9605, 'grad_norm': 3.7698702812194824, 'learning_rate': 0.001957817324187987, 'epoch': 1.2}\n",
      "{'loss': 1.9355, 'grad_norm': 5.6010870933532715, 'learning_rate': 0.0019480091799562705, 'epoch': 1.3}\n",
      "{'loss': 1.9555, 'grad_norm': 4.9506449699401855, 'learning_rate': 0.0019372067050063438, 'epoch': 1.4}\n",
      "{'loss': 2.1249, 'grad_norm': 4.4947052001953125, 'learning_rate': 0.0019254212296427042, 'epoch': 1.5}\n",
      "{'loss': 1.4429, 'grad_norm': 5.161564826965332, 'learning_rate': 0.0019126651152015402, 'epoch': 1.6}\n",
      "{'loss': 1.8356, 'grad_norm': 3.340679407119751, 'learning_rate': 0.0018989517410853954, 'epoch': 1.7}\n",
      "{'loss': 1.4811, 'grad_norm': 3.9123029708862305, 'learning_rate': 0.0018842954907300237, 'epoch': 1.8}\n",
      "{'loss': 1.5675, 'grad_norm': 3.8736701011657715, 'learning_rate': 0.0018687117365181513, 'epoch': 1.9}\n",
      "{'loss': 1.5518, 'grad_norm': 5.282261848449707, 'learning_rate': 0.0018522168236559692, 'epoch': 2.0}\n",
      "{'loss': 1.3892, 'grad_norm': 23.796457290649414, 'learning_rate': 0.0018348280530292712, 'epoch': 2.1}\n",
      "{'loss': 1.5852, 'grad_norm': 5.1641035079956055, 'learning_rate': 0.001816563663057211, 'epoch': 2.2}\n",
      "{'loss': 1.7168, 'grad_norm': 3.454280376434326, 'learning_rate': 0.0017974428105627207, 'epoch': 2.3}\n",
      "{'loss': 1.5162, 'grad_norm': 7.118550777435303, 'learning_rate': 0.0017774855506796495, 'epoch': 2.4}\n",
      "{'loss': 1.5636, 'grad_norm': 5.879214763641357, 'learning_rate': 0.0017567128158176952, 'epoch': 2.5}\n",
      "{'loss': 1.4284, 'grad_norm': 3.664971113204956, 'learning_rate': 0.0017351463937072004, 'epoch': 2.6}\n",
      "{'loss': 1.6284, 'grad_norm': 8.761506080627441, 'learning_rate': 0.0017128089045468293, 'epoch': 2.7}\n",
      "{'loss': 1.698, 'grad_norm': 6.7574872970581055, 'learning_rate': 0.0016897237772781045, 'epoch': 2.8}\n",
      "{'loss': 1.4369, 'grad_norm': 5.6306986808776855, 'learning_rate': 0.0016659152250116812, 'epoch': 2.9}\n",
      "{'loss': 1.6606, 'grad_norm': 9.56978702545166, 'learning_rate': 0.00164140821963114, 'epoch': 3.0}\n",
      "{'loss': 1.3039, 'grad_norm': 9.3112154006958, 'learning_rate': 0.0016162284656009273, 'epoch': 3.1}\n",
      "{'loss': 1.5689, 'grad_norm': 6.763780117034912, 'learning_rate': 0.0015904023730059227, 'epoch': 3.2}\n",
      "{'loss': 1.8078, 'grad_norm': 6.2871479988098145, 'learning_rate': 0.0015639570298509064, 'epoch': 3.3}\n",
      "{'loss': 1.4217, 'grad_norm': 5.482674598693848, 'learning_rate': 0.0015369201736489839, 'epoch': 3.4}\n",
      "{'loss': 1.0959, 'grad_norm': 7.330221652984619, 'learning_rate': 0.001509320162328763, 'epoch': 3.5}\n",
      "{'loss': 1.3762, 'grad_norm': 6.949548721313477, 'learning_rate': 0.001481185944490805, 'epoch': 3.6}\n",
      "{'loss': 1.2211, 'grad_norm': 3.7034459114074707, 'learning_rate': 0.0014525470290445391, 'epoch': 3.7}\n",
      "{'loss': 1.5127, 'grad_norm': 4.241636753082275, 'learning_rate': 0.0014234334542574906, 'epoch': 3.8}\n",
      "{'loss': 1.1985, 'grad_norm': 5.08134651184082, 'learning_rate': 0.0013938757562492873, 'epoch': 3.9}\n",
      "{'loss': 1.2316, 'grad_norm': 4.6223835945129395, 'learning_rate': 0.0013639049369634877, 'epoch': 4.0}\n",
      "{'loss': 0.9168, 'grad_norm': 5.909273624420166, 'learning_rate': 0.0013335524316508208, 'epoch': 4.1}\n",
      "{'loss': 1.1954, 'grad_norm': 4.511951923370361, 'learning_rate': 0.0013028500758979506, 'epoch': 4.2}\n",
      "{'loss': 1.0541, 'grad_norm': 2.7139010429382324, 'learning_rate': 0.001271830072236343, 'epoch': 4.3}\n",
      "{'loss': 1.2399, 'grad_norm': 3.51594877243042, 'learning_rate': 0.0012405249563662538, 'epoch': 4.4}\n",
      "{'loss': 0.8411, 'grad_norm': 5.342329025268555, 'learning_rate': 0.0012089675630312753, 'epoch': 4.5}\n",
      "{'loss': 1.1872, 'grad_norm': 3.6142661571502686, 'learning_rate': 0.0011771909915792229, 'epoch': 4.6}\n",
      "{'loss': 1.1599, 'grad_norm': 4.559562683105469, 'learning_rate': 0.0011452285712454905, 'epoch': 4.7}\n",
      "{'loss': 0.8903, 'grad_norm': 4.04403018951416, 'learning_rate': 0.0011131138261952845, 'epoch': 4.8}\n",
      "{'loss': 0.9287, 'grad_norm': 4.143606185913086, 'learning_rate': 0.0010808804403614042, 'epoch': 4.9}\n",
      "{'loss': 0.9729, 'grad_norm': 4.553323745727539, 'learning_rate': 0.0010485622221144484, 'epoch': 5.0}\n",
      "{'loss': 0.8915, 'grad_norm': 5.692973613739014, 'learning_rate': 0.0010161930688025015, 'epoch': 5.1}\n",
      "{'loss': 0.6934, 'grad_norm': 3.0388617515563965, 'learning_rate': 0.0009838069311974985, 'epoch': 5.2}\n",
      "{'loss': 0.6125, 'grad_norm': 4.065757751464844, 'learning_rate': 0.000951437777885552, 'epoch': 5.3}\n",
      "{'loss': 0.8622, 'grad_norm': 3.4590630531311035, 'learning_rate': 0.0009191195596385959, 'epoch': 5.4}\n",
      "{'loss': 0.5959, 'grad_norm': 4.768935680389404, 'learning_rate': 0.0008868861738047158, 'epoch': 5.5}\n",
      "{'loss': 0.8683, 'grad_norm': 2.859670639038086, 'learning_rate': 0.00085477142875451, 'epoch': 5.6}\n",
      "{'loss': 0.7422, 'grad_norm': 1.5869680643081665, 'learning_rate': 0.0008228090084207773, 'epoch': 5.7}\n",
      "{'loss': 0.7382, 'grad_norm': 3.002058982849121, 'learning_rate': 0.000791032436968725, 'epoch': 5.8}\n",
      "{'loss': 0.9526, 'grad_norm': 4.244437217712402, 'learning_rate': 0.0007594750436337467, 'epoch': 5.9}\n",
      "{'loss': 0.6623, 'grad_norm': 2.8997817039489746, 'learning_rate': 0.0007281699277636571, 'epoch': 6.0}\n",
      "{'loss': 0.5263, 'grad_norm': 7.061781883239746, 'learning_rate': 0.0006971499241020494, 'epoch': 6.1}\n",
      "{'loss': 0.418, 'grad_norm': 4.360199928283691, 'learning_rate': 0.0006664475683491796, 'epoch': 6.2}\n",
      "{'loss': 0.443, 'grad_norm': 2.072559118270874, 'learning_rate': 0.0006360950630365126, 'epoch': 6.3}\n",
      "{'loss': 0.4843, 'grad_norm': 2.8976311683654785, 'learning_rate': 0.0006061242437507131, 'epoch': 6.4}\n",
      "{'loss': 0.4594, 'grad_norm': 2.910229444503784, 'learning_rate': 0.0005765665457425102, 'epoch': 6.5}\n",
      "{'loss': 0.5169, 'grad_norm': 2.061490535736084, 'learning_rate': 0.0005474529709554612, 'epoch': 6.6}\n",
      "{'loss': 0.4994, 'grad_norm': 3.716632127761841, 'learning_rate': 0.0005188140555091949, 'epoch': 6.7}\n",
      "{'loss': 0.4832, 'grad_norm': 3.4484829902648926, 'learning_rate': 0.0004906798376712373, 'epoch': 6.8}\n",
      "{'loss': 0.5005, 'grad_norm': 3.2051892280578613, 'learning_rate': 0.0004630798263510162, 'epoch': 6.9}\n",
      "{'loss': 0.5628, 'grad_norm': 3.0157065391540527, 'learning_rate': 0.0004360429701490934, 'epoch': 7.0}\n",
      "{'loss': 0.3003, 'grad_norm': 2.8669912815093994, 'learning_rate': 0.00040959762699407763, 'epoch': 7.1}\n",
      "{'loss': 0.2918, 'grad_norm': 0.7275216579437256, 'learning_rate': 0.00038377153439907266, 'epoch': 7.2}\n",
      "{'loss': 0.2131, 'grad_norm': 1.057610034942627, 'learning_rate': 0.0003585917803688603, 'epoch': 7.3}\n",
      "{'loss': 0.2657, 'grad_norm': 3.9068713188171387, 'learning_rate': 0.00033408477498831913, 'epoch': 7.4}\n",
      "{'loss': 0.2609, 'grad_norm': 1.7849806547164917, 'learning_rate': 0.00031027622272189573, 'epoch': 7.5}\n",
      "{'loss': 0.2592, 'grad_norm': 2.806206464767456, 'learning_rate': 0.00028719109545317104, 'epoch': 7.6}\n",
      "{'loss': 0.2855, 'grad_norm': 3.053255558013916, 'learning_rate': 0.0002648536062927999, 'epoch': 7.7}\n",
      "{'loss': 0.2993, 'grad_norm': 2.0555951595306396, 'learning_rate': 0.00024328718418230468, 'epoch': 7.8}\n",
      "{'loss': 0.2676, 'grad_norm': 2.5962347984313965, 'learning_rate': 0.0002225144493203509, 'epoch': 7.9}\n",
      "{'loss': 0.2538, 'grad_norm': 1.5324558019638062, 'learning_rate': 0.0002025571894372794, 'epoch': 8.0}\n",
      "{'loss': 0.1632, 'grad_norm': 1.6611003875732422, 'learning_rate': 0.00018343633694278895, 'epoch': 8.1}\n",
      "{'loss': 0.1664, 'grad_norm': 1.6591964960098267, 'learning_rate': 0.00016517194697072903, 'epoch': 8.2}\n",
      "{'loss': 0.159, 'grad_norm': 2.1078875064849854, 'learning_rate': 0.0001477831763440308, 'epoch': 8.3}\n",
      "{'loss': 0.1561, 'grad_norm': 1.1221632957458496, 'learning_rate': 0.00013128826348184885, 'epoch': 8.4}\n",
      "{'loss': 0.1739, 'grad_norm': 0.5618546009063721, 'learning_rate': 0.00011570450926997656, 'epoch': 8.5}\n",
      "{'loss': 0.1335, 'grad_norm': 1.129927396774292, 'learning_rate': 0.00010104825891460479, 'epoch': 8.6}\n",
      "{'loss': 0.1189, 'grad_norm': 0.7048669457435608, 'learning_rate': 8.733488479845996e-05, 'epoch': 8.7}\n",
      "{'loss': 0.153, 'grad_norm': 0.3012288510799408, 'learning_rate': 7.457877035729587e-05, 'epoch': 8.8}\n",
      "{'loss': 0.1782, 'grad_norm': 2.059713363647461, 'learning_rate': 6.279329499365649e-05, 'epoch': 8.9}\n",
      "{'loss': 0.1424, 'grad_norm': 1.3311607837677002, 'learning_rate': 5.199082004372957e-05, 'epoch': 9.0}\n",
      "{'loss': 0.1078, 'grad_norm': 0.3839048743247986, 'learning_rate': 4.218267581201296e-05, 'epoch': 9.1}\n",
      "{'loss': 0.0919, 'grad_norm': 0.3895263969898224, 'learning_rate': 3.337914968738887e-05, 'epoch': 9.2}\n",
      "{'loss': 0.0976, 'grad_norm': 0.14311544597148895, 'learning_rate': 2.5589475353073986e-05, 'epoch': 9.3}\n",
      "{'loss': 0.0948, 'grad_norm': 2.1730546951293945, 'learning_rate': 1.882182310176095e-05, 'epoch': 9.4}\n",
      "{'loss': 0.0985, 'grad_norm': 0.3513700067996979, 'learning_rate': 1.3083291266109298e-05, 'epoch': 9.5}\n",
      "{'loss': 0.099, 'grad_norm': 0.11605008691549301, 'learning_rate': 8.379898773574923e-06, 'epoch': 9.6}\n",
      "{'loss': 0.1239, 'grad_norm': 0.8848090171813965, 'learning_rate': 4.7165788333860535e-06, 'epoch': 9.7}\n",
      "{'loss': 0.1117, 'grad_norm': 0.7402138710021973, 'learning_rate': 2.0971737622883515e-06, 'epoch': 9.8}\n",
      "{'loss': 0.1005, 'grad_norm': 0.1756032258272171, 'learning_rate': 5.244309544850667e-07, 'epoch': 9.9}\n",
      "{'loss': 0.1107, 'grad_norm': 0.4537005126476288, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 992.0218, 'train_samples_per_second': 2.016, 'train_steps_per_second': 1.008, 'train_loss': 0.9669905952215194, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "num_train_epochs = 10\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,      \n",
    "    per_device_train_batch_size=1,          \n",
    "    gradient_accumulation_steps=2,          \n",
    "    optim=\"paged_adamw_32bit\",              \n",
    "    save_steps=0,                           \n",
    "    logging_steps=10,                       \n",
    "    learning_rate=2e-3,                     \n",
    "    weight_decay=0.001,                     \n",
    "    fp16=False,                            \n",
    "    bf16=False,                             \n",
    "    max_grad_norm=0.3,                     \n",
    "    max_steps=-1,                           \n",
    "    warmup_ratio=0.03,                      \n",
    "    group_by_length=True,                   \n",
    "    lr_scheduler_type=\"cosine\",           \n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=new_df,\n",
    "    peft_config=peft_config,                \n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=100,                    \n",
    "    tokenizer=tokenizer,                   \n",
    "    args=training_arguments,                \n",
    "    packing=False,                          \n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca0ad5-8d52-49d5-af87-3d6ed0287c8c",
   "metadata": {},
   "source": [
    "# Testing the fine tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "574a8429-0258-458a-b2e1-3c2be03d068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INST] I have a multi-line string literal that I want to do an operation on each line, like so: inputString = '''Line 1 Line 2 Line 3''' I want to do something like the following: for line in inputString: doStuff() [/INST] You can simply use str.splitlines. Besides the advantage mentioned by @efotinis of optionally including the newline character in the split result when called with a True argument, splitlines() handles newlines properly, unlike split(\"\\n\"). \\n, in Python, represents a Unix line-break (ASCII decimal code 10), independently from the platform where you run it. However, the linebreak representation is platform-dependent. On Windows, \\n is two characters, CR and LF (ASCII decimal codes 13 and 10, AKA \\r and \\n), while on any modern Unix (including OS X), it's the single character LF. print, for example, works correctly even if you have a string with line endings that don't match your platform: >>> print \" a \\n b \\r\\n c \" a b c However, explicitly splitting on \"\\n\", will yield platform-dependent behaviour: >>> \" a \\n b \\r\\n c \".split(\"\\n\") [' a ', ' b \\r', ' c '] Even if you use os.linesep, it will only split according to the newline separator on your platform, and will fail if you're processing text created in other platforms, or with a bare \\n: >>> \" a \\n b \\r\\n c \".split(os.linesep) [' a \\n b ', ' c '] splitlines solves all these problems: >>> \" a \\n b \\r\\n c \".splitlines() [' a ', ' b ', ' c '] Reading files in text mode partially mitigates the newline representation problem, as it converts Python's \\n into the platform's newline representation. However, text mode only exists on Windows.\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"I have a multi-line string literal that I want to do an operation on each line, like so: inputString = '''Line 1 Line 2 Line 3''' I want to do something like the following: for line in inputString: doStuff()\"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a12d8-9df9-4ab6-967d-b7a4d96c7061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
